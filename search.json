[{"title":"多边形的扫描转换与区域填充算法","date":"2020-11-09T14:33:08.000Z","url":"/2020/11/09/%E5%A4%9A%E8%BE%B9%E5%BD%A2%E7%9A%84%E6%89%AB%E6%8F%8F%E8%BD%AC%E6%8D%A2%E4%B8%8E%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85%E7%AE%97%E6%B3%95/","tags":[["图形学","/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"],["数据结构","/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"]],"categories":[["Assign","/categories/Assign/"]],"content":"多边形的扫描转换与区域填充算法为了完成这次算法作业, 需要C++ 图形编程. 我选择了OpenGL. 配置起来有点麻烦, 又遇到了一些Bug, 最后索性用了GL里面的一个库, glut.h 进行尝试. Cheating CodeGLUT的学习用的Clion, 配置了库, 但是可能有问题, 至少Glut是能正常使用的 随便找的一篇博客配置OpenGL 随便找到的一篇GLUT Tutorial TODO (demo: 用鼠标选点, [ENTER] 输入选好的点, 然后连线, 输出点的坐标)"},{"title":"Transformer模型学习笔记","date":"2020-11-08T17:22:25.000Z","url":"/2020/11/09/Transformer%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","tags":[["Machine Learning","/tags/Machine-Learning/"],["NLP","/tags/NLP/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"]],"content":"Transformer 模型学习笔记论文地址Attention Is All You NeedNice Blog for illustrating Transformer Modelseq2seq with attention提出的特点 RNN无法并行地去处理一个序列, 因为每个hidden state $h_i$都是依赖于上一个hidden state $h_{i-1}$以及input. 所以就要一个step接一个step的去循环, 对于很长的序列训练起来就很耗时. Transformer 模型利用Attention机制去捕获全局的input与output之间的依赖性, 实质上就是将整条序列看作一个input向量, 也就避免了循环神经网络中的”循环“. 实质上算是对RNN循环过程的一个展开吧. 完全使用Attention机制, 没有使用序列对齐的循环(sequence-aligned recurrence)或者卷积层 背景知识Self attention的优势 Gated RNNs 虽然在结构上能够记录前 $n - 1$ 个token的信息, 但实际上, 随着序列变长, 最早的token信息会变得很少, 这就会失去他的准确性, 这在翻译任务中就显得非常要命, 例如, 在English-to-French的翻译里, output的第一个词大概率是依赖于input开始的部分, 这样很可能会得到很差的结果. 而transformer模型似乎是靠着更大的存储和算力来强行将前 $n$ 个token利用attention融合起来. 这样看来, 似乎是对症下药, 实验上也得到了很好的结果. 而具体的self attention会在模型架构中介绍 Word EmbeddingBlog for introducing Word2Vec TODO 学习Word2Vec 模型架构 在上面的架构图中包含encoder以及decoder的结构transformer model完成的翻译任务实际上还是encoder-decoder的一个结构, TODO (与seq2seq with attention 有很多重合的部分, 找出核心的不同, 理解output(shift)为啥能作为init decoder input, 和怎么mask)"},{"title":"我的第一篇文章","date":"2020-11-08T11:03:14.000Z","url":"/2020/11/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/","tags":[["test","/tags/test/"]],"categories":[["test","/categories/test/"]],"content":"this is my first blog here"},{"title":"Hello World","date":"2020-11-08T10:48:32.418Z","url":"/2020/11/08/hello-world/","categories":[[" ",""]],"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post More info: Writing Run server More info: Server Generate static files More info: Generating Deploy to remote sites More info: Deployment"},{"title":"search","date":"2020-11-08T12:14:33.000Z","url":"/search/index.html","categories":[[" ",""]]},{"title":"about","date":"2020-11-08T12:32:12.000Z","url":"/about/index.html","categories":[[" ",""]],"content":"Wu Shiguang.Taishan College, Shandong University"},{"title":"tags","date":"2020-11-08T12:30:26.000Z","url":"/tags/index.html","categories":[[" ",""]]},{"title":"categories","date":"2020-11-08T17:39:56.000Z","url":"/categories/index.html","categories":[[" ",""]]}]