[{"title":"Word2Vec","date":"2020-11-10T16:03:16.000Z","url":"/2020/11/11/Word2Vec/","tags":[["Machine Learning","/tags/Machine-Learning/"],["NLP","/tags/NLP/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"]],"content":"beginning其实用向量来表示一个单词是很常见的, 毕竟方便一个算法或模型去表示一个单词. 但表示的方法很重要, 最完美的Embedding就是能够包含单词的语义, 相似语义的单词的表示越接近, 反之表示的向量差距越大. 下面一步步的去完成我们的目标. distance怎么来叙述两个单词的embedding是相近的呢? 容易想到对于两个向量, 我们有一个适用于 $n$ 维空间上的一个夹角公式. for $\\vec{a}$, $\\vec{b}$ as two embedding vectors of different words. the cosine similarity is$$Cosine Similarity(\\vec{a}, \\vec{b}) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}|\\cdot|\\vec{b}|}$$ 这个评判函数相对简单, 也非常有效. 对于不同的问题, 我们也可以选择不同的距离函数. modelingembedding都是为了更好的完成一个task, 那么下面先来看一个简单的任务, 在完成任务的同时, 来获得一个适合它的embedding. predicting next word根据语境预测下一个单词是什么. 这是一个在日常生活中非常常见的任务.简单来做, 就是分为三步, 输入一个单词, 先获得对应的embedding 利用embedding去预测下一个单词的embedding 根据embedding映射到对应的单词, 然后输出 数据集从哪来呢 一般是Wikipedia的文章等等 构造许多定长的input - output对, 做法是window sliding with a fixed length, 前面的都是input, 最后一个单词是output problem这么做有两个问题. 预测一个单词怎么能只看前几个单词呢 最后有个 $n \\times d$ , (其中 $n$ 表示Vacab的大小, $d$ 表示embedding的大小) , 这算起来太费时了 problem solving 取样时不仅取前面的, 把后面的也取了. 这叫skip gram. 这就成了Clozing了 换个方法, 把他改成预测谁是neighbor, 输出可能是neighbor的概率 computational problem咋整. 再调整我们的任务 我们改成更小规模的模型, 给他一个input, 和一堆output, 然后输出这些output分别是不是他的neighbor 那有可能我们训出了一个人工智障, 他告诉我们这些全都是neighbor, 我们好像也没法反驳. 给他来个negative sample, 告诉model他们不是neighbor, 这样model就成了个logistics regression模型, 规模小了很多 但咱采样的时候怎么知道他们到底有没有可能是neighbor, 我们手上的数据集肯定是不完备的呀 随机设为negative…… 问题基本解决了, 下面看具体流程 Word2Vec 我们训两个embeddings, 叫做Embedding和Context, 分别为input和一组outputs做project. 获得两组embedding: $\\vec{input}$ 和 $\\vec{output}$. 这个outputs肯定就是一些真正的neighbor(通过skip gram window sliding选出来的)加上一些随机的negative sample. input $\\to$ Embedding $\\to$ $\\vec{input}$ outputs $\\to$ Context $\\to$ a set of $\\vec{output}$s 然后点乘, 来个softmax, 获得这么个概率, 根据误差不断训 最后, 训得差不多了, 扔掉context, 这个Embedding就是炼出来的丹 last 语义相近的那些单词, 被认为是所在的Context是相似的. 越相似的词点乘上Context, 得到的结果肯定都很接近正确结果. 这个window sliding 的length一般设成5, 越小的话得到的embedding划的越细, 就是说相近的embedding的单词所在的context几乎一样, 但要注意, 反义词很多时候也是这样, 把length设大点就能区分更多语境 Endreference a blog 还没看过原paper和代码, 估计等以后了 "},{"title":"多边形的扫描转换与区域填充算法","date":"2020-11-09T14:33:08.000Z","url":"/2020/11/09/%E5%A4%9A%E8%BE%B9%E5%BD%A2%E7%9A%84%E6%89%AB%E6%8F%8F%E8%BD%AC%E6%8D%A2%E4%B8%8E%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85%E7%AE%97%E6%B3%95/","tags":[["图形学","/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"],["数据结构","/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"]],"categories":[["Assign","/categories/Assign/"]],"content":"多边形的扫描转换与区域填充算法为了完成这次算法作业, 需要C++ 图形编程. 我选择了OpenGL. 配置起来有点麻烦, 又遇到了一些Bug, 最后索性用了GL里面的一个库, glut.h 进行尝试. Cheating CodeGLUT的学习用的Clion, 配置了库, 但是可能有问题, 至少Glut是能正常使用的 随便找的一篇博客配置OpenGL 随便找到的一篇GLUT Tutorial 捣腾半天发现其实我需要的功能并不是很复杂, 也没用到太多glut.h更深入的东西. 了解了glut基本的结构以后还是比较容易看懂整体的流程的. main() myInit() recall to be continue… 算法代码之后再说…."},{"title":"Transformer模型学习笔记","date":"2020-11-08T17:22:25.000Z","url":"/2020/11/09/Transformer%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","tags":[["Machine Learning","/tags/Machine-Learning/"],["NLP","/tags/NLP/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"]],"content":"Transformer 模型学习笔记论文地址Attention Is All You NeedNice Blog for illustrating Transformer Modelseq2seq with attention提出的特点 RNN无法并行地去处理一个序列, 因为每个hidden state $h_i$都是依赖于上一个hidden state $h_{i-1}$以及input. 所以就要一个step接一个step的去循环, 对于很长的序列训练起来就很耗时. Transformer 模型利用Attention机制去捕获全局的input与output之间的依赖性, 实质上就是将整条序列看作一个input向量, 也就避免了循环神经网络中的”循环“. 实质上算是对RNN循环过程的一个展开吧. 完全使用Attention机制, 没有使用序列对齐的循环(sequence-aligned recurrence)或者卷积层 背景知识Self attention的优势 Gated RNNs 虽然在结构上能够记录前 $n - 1$ 个token的信息, 但实际上, 随着序列变长, 最早的token信息会变得很少, 这就会失去他的准确性, 这在翻译任务中就显得非常要命, 例如, 在English-to-French的翻译里, output的第一个词大概率是依赖于input开始的部分, 这样很可能会得到很差的结果. 而transformer模型似乎是靠着更大的存储和算力来强行将前 $n$ 个token利用attention融合起来. 这样看来, 似乎是对症下药, 实验上也得到了很好的结果. 而具体的self attention会在模型架构中介绍 Word EmbeddingBlog for introducing Word2Vec我自己的对Word2Vec的学习笔记 模型架构其实Transformer模型的改进应该去对比seq2seq with attention模型 首先在seq2seq模型里,用的是最初的encoder-decoder思想, 拿翻译任务来说, input就是一个句子, encoder需要一个单词一个单词(token)的去encode, 也就是扔进一个RNNs, 每次得到一个hidden state, 句子全输进去了, 最后得到的hidden state就可以作为整个句子的representation. 这个思想很简单, 给我的是不定长的, 那我就把他搞成一个固定长度的hidden state. 之后拿着这个representation当作decoder的input, 再一个单词一个单词的预测. 前后都是RNN. 乍一看貌似还是挺好的, 但问题是RNN的记忆机制和梯度问题解决的不是那么好, 句子长了前面的单词他就忘记了. 而且翻译这个任务也确实需要一种attention ,output的某一个部分会很大程度依赖于input中的一部分. 加了attention的seq2seq似乎就考虑到了这种依赖关系, attention机制也很好的做到了这一点. 至于整体做法, 上面不是说encoder内不断地生成hidden state吗, 那我们就把它们全都取出来作为decoder的input, 这样就不用太担心记忆的问题了, 毕竟你把它们都拿出来了. 然后每个output预测值会利用attention机制去给这些hidden state附上注意力的权重, 来更好地完成任务. 再到transformer. 这样纵向的来看, 似乎改进的地方确实如原论文所讲, 去掉了所有的循环连接. 完全用attention来解决. 这样做就需要在一些地方进行调整. Attention虽然字面上讲的attention, 似乎是个很熟悉的概念, 但实际在具体的实现上是另一种更加抽象的机制. 一般来讲,这个问题是想输入两种序列, $S= {S_1, S_2, S_3 \\dots S_n }$ 以及 $T ={T_1, T_2, T_3 \\dots T_m }$ 我想输出$T$ 对于$S$ 的attention, 具体的可以说就是一个function, $Attention_{S_i}(T_j),\\quad i= 1,2\\dots n$ ,然后有$$\\sum_{j} Attention_{S_i}(T_j)=1, \\quad i = 1,2,\\dots n$$就可以,值越大就越重要. 那么整个T对于 $S_i$ 的representation就是 $T_{s_i} = \\sum_{j} Attention_{S_i} (T_j)\\times Value(T_j)$ 这么一个加权平均 这里的元素就是一些embedding, 一些向量. Attention的求法类似于一种查询. 每个 $S_i$ 都会对应一个query向量 $\\boldsymbol{q_i}$ 每个 $T_i$ 又对应一个键值 $\\boldsymbol{k_i}$ 以供”查询”, 查到的结果就是两个向量的点积 $a_{ij} = \\boldsymbol{q_i} \\cdot \\boldsymbol{k_j}$ , (假设这里的两个向量的维数都是 $d_k$ ). 最后的Attention就是再加上一个softmax$$Attention_{S_i}(T_j) = \\frac{e^{\\boldsymbol{q_i} \\cdot \\boldsymbol{k_j}^T}}{\\sum_j e^{\\boldsymbol{q_i} \\cdot \\boldsymbol{k_j}^T}}$$ 每个 $T_j$ 又会对应一个Value向量 $v_j$ (维度可以和前面两个向量不同, 记为$d_v$)用以获得representation. 最后得到的就是$$Representation_{for, S_i} = softmax(\\boldsymbol{q}{1\\times d_k}\\cdot \\boldsymbol K_{m \\times d_k}^T)_{1\\times m} \\boldsymbol V{m\\times d_k}$$ 进一步可以获得 $T$ 对 $S$ 的表示$$softmax(\\boldsymbol{Q}{n\\times d_k}\\cdot \\boldsymbol K_{m \\times d_k}^T)_{n\\times m} \\boldsymbol V{m\\times d_v}$$为了”having more stable gradients” , 在$\\boldsymbol{Q}\\cdot \\boldsymbol K^T$这里还要除以一个因子, 默认是 $\\sqrt{d_k}$ , 然后又变成了$$softmax(\\frac{\\boldsymbol{Q}\\cdot \\boldsymbol K^T}{\\sqrt{d_k}}) \\boldsymbol V$$有人要问了, 你说的这些query,key和value向量都咋求呢. 用三个线性映射(矩阵) $W^Q,W^K,W^V$ 线性映射哪来的呢 学出来的 a beast with multihead这样一组attention可能注意力太集中, 看不全, 那我们就让他有多个”头”, 注意力分散点, 看得更全 tensor2tensor上有个示例, 演示的就是attention 一个比较经典的例子 翻译句子 The animal didn’t cross the street because it was too tired 这里的 it 应该代指 The animal, 但是对模型来说, 他也可能是说the street. 这里可以看到, 模型确实被it的代指给弄晕了,但还好animal处的颜色比street的地方要深,说明他的权值要大 但并不是所有的attention都能学到对应的部分. 解决办法就是, 我们用多个attention去拼接成最终想要的representation. 具体的, 我们得到的value向量不是 $d_v$ 维的吗, 假设我们有 $h$ 个head, 那么就把向量分为$h$ 个维度为 $d_v / h$ 的向量, 每个用各自的线性映射得到 $h$ 组不同的 $\\boldsymbol{Q,, K,, V_{m \\times (d_v/h)}}$ 去求各自的value^pic 最后一般还会再乘上一个矩阵 $W^O$,来得到最后的输出^pic 用上了多个head, 我们就能同时去关注不同的区域, 获得更准确的表述 整体架构前面花了较长篇幅讲了Attention机制, 这里再看下它是如何被用在Transformer中的 在上面的架构图中包含encoder以及decoder的结构^model 左边是encoder, 他的特点就是直接将整条序列直接放进网络层中. 工作流程就是, 首先把要处理的序列input输入, 再获得它的embeddings. 然后依次进入每个encoder层, "},{"title":"我的第一篇文章","date":"2020-11-08T11:03:14.000Z","url":"/2020/11/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/","tags":[["test","/tags/test/"]],"categories":[["test","/categories/test/"]],"content":"this is my first blog here"},{"title":"Hello World","date":"2020-11-08T10:48:32.418Z","url":"/2020/11/08/hello-world/","categories":[[" ",""]],"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post More info: Writing Run server More info: Server Generate static files More info: Generating Deploy to remote sites More info: Deployment"},{"title":"search","date":"2020-11-08T12:14:33.000Z","url":"/search/index.html","categories":[[" ",""]]},{"title":"about","date":"2020-11-08T12:32:12.000Z","url":"/about/index.html","categories":[[" ",""]],"content":"Wu Shiguang.Taishan College, Shandong University"},{"title":"tags","date":"2020-11-08T12:30:26.000Z","url":"/tags/index.html","categories":[[" ",""]]},{"title":"categories","date":"2020-11-08T17:39:56.000Z","url":"/categories/index.html","categories":[[" ",""]]}]