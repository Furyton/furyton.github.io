[{"title":"Transformer模型学习笔记","date":"2020-11-08T17:22:25.000Z","url":"/2020/11/09/Transformer%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","tags":[["Machine Learning","/tags/Machine-Learning/"],["NLP","/tags/NLP/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"]],"content":"Transformer 模型学习笔记论文地址Attention Is All You Need提出的特点 RNN无法并行地去处理一个序列, 因为每个hidden state $h_i$都是依赖于上一个hidden state $h_{i-1}$以及input. 所以就要一个step接一个step的去循环, 对于很长的序列训练起来就很耗时. Transformer 模型利用Attention机制去捕获全局的input与output之间的依赖性, 实质上就是将整条序列看作一个input向量, 也就避免了循环神经网络中的”循环“. 实质上算是对RNN循环过程的一个展开吧. 完全使用Attention机制, 没有使用序列对齐的循环(sequence-aligned recurrence)或者卷积层 模型架构 to be continue…."},{"title":"我的第一篇文章","date":"2020-11-08T11:03:14.000Z","url":"/2020/11/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/","tags":[["test","/tags/test/"]],"categories":[["test","/categories/test/"]],"content":"this is my first blog here"},{"title":"Hello World","date":"2020-11-08T10:48:32.418Z","url":"/2020/11/08/hello-world/","categories":[[" ",""]],"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post More info: Writing Run server More info: Server Generate static files More info: Generating Deploy to remote sites More info: Deployment"},{"title":"search","date":"2020-11-08T12:14:33.000Z","url":"/search/index.html","categories":[[" ",""]]},{"title":"about","date":"2020-11-08T12:32:12.000Z","url":"/about/index.html","categories":[[" ",""]],"content":"Wu Shiguang.Taishan College, Shandong University"},{"title":"tags","date":"2020-11-08T12:30:26.000Z","url":"/tags/index.html","categories":[[" ",""]]},{"title":"categories","date":"2020-11-08T17:39:56.000Z","url":"/categories/index.html","categories":[[" ",""]]}]