[{"title":"Combinatorics course note - Turan problem","date":"2021-03-06T14:17:30.000Z","url":"/2021/03/06/Combinatorics-course-note-Turan-problem/","tags":[["note","/tags/note/"],["Study","/tags/Study/"],["Graph Theory","/tags/Graph-Theory/"],["Turan problem","/tags/Turan-problem/"]],"categories":[["extend","/categories/extend/"],["Graph Theory","/categories/extend/Graph-Theory/"]],"content":"课程链接 Course Link 闲来无事听的课，看看自己能坚持多久 习题解答并不能保证正确性 Turan Problem 就是求解一个数 extremal number of a graph H \\[ ex(n,H)=max\\{e(G) : |G|=n \\, and \\, G \\;is \\; H-free\\} \\] 在顶点数量限制下，不包含子图H，最大化边的数量 第一个结果是triangle-free的extremal number (Mantel 1907) \\[ e(G)\\le ex(n,K_3)= \\left \\lfloor n^2/4 \\right \\rfloor \\] 最大的图是两边各有 \\(\\left \\lfloor n/2 \\right \\rfloor\\) 个点的二分图 exercise ex1: 选择n个无理数，怎么才能\\(max\\{\\#(x_i,x_j):x_i+x_j \\in Q\\}\\) A：转化成图的问题，两个数和如果是有理数那么就有一条边，否则没有。注意到图中不能有三角形，那么根据上面的定理就解决了。 ex2: 证明对于任意一个k个顶点的树 T, \\(ex(n,T)\\le kn\\) A: 如果图G有\\(kn\\)个边，那么一定能够找到它的一个子图G'，其最小度数大于等于k。如果子图中有一个点v度数小于k，那么把他去掉，最后的度数为 \\[ \\frac{\\sum_{i=1}^{|G&#39;|} d(i)-2d(v)}{|G|-1}\\ge \\frac{2e(G&#39;)}{|G|}\\gt k\\] , 且这个平均值在一直增加，最终一定可以得到所需的子图 T可以表示成一个序列\\([v_1,v_2,\\dots,v_k]\\) ，对每个\\(v_i\\) ，只有唯一个点\\(v_j \\in [v_1,\\dots,v_{i-1}]\\) ，\\(v_i\\)和\\(v_j\\)相连。由此可以构造性的证明，上述得到的子图G'包含所有k顶点的树。 extremal structure turan r-partite turan graph =&gt; \\(T_r(n)\\) \\[ ex(n,K_{r+1})=e(T_r(n))=(1-\\frac{1}{r})\\frac{n^2}{2}-O(r), \\quad r\\in N,r\\ge2 \\] stability let \\(\\epsilon &gt; 0\\), exists \\(\\delta &gt; 0\\), G be an n-vertex \\(K_{k+1}\\) -free graph, if \\[ e(G)\\ge ex(n,K_{r+1})-\\delta n^2 \\] then G can be changed to \\(T_r(n)\\) by altering at most \\(\\epsilon n^2\\) adjacencies Motzkin-Straus \\[ f_G(x)=x^TA_Gx=\\sum_{i,j=1\\wedge v_iv_j\\in e(G)}^{n}x_ix_j \\] \\(S_n\\) 是概率单纯形。 \\(\\boldsymbol{x}\\) 是n维的，所以可以看作是\\(V(G)\\)上的一个权重。 有\\(f_G(\\boldsymbol{x})=2e(G_x)\\) , \\(e(G_x)\\) 是加权边的和，权重是两个顶点权重的乘积，。\\(\\boldsymbol{x}\\) in \\(S_n\\) 定理内容： 图G，\\(cl(G)=k\\)，那么对于任意一个，\\(\\boldsymbol{x}\\) in \\(S_n\\)都有一个\\(\\boldsymbol{y}\\in S_n\\)， \\[ \\left\\{\\begin{matrix} f_G(y)\\ge f_G(x)\\\\ supp(y)=K_k \\end{matrix}\\right. \\] 进一步可知，\\(f_G(\\boldsymbol{x})\\le \\frac{k-1}{k}\\) 实质上是给了一个最优化问题的解 证明略 exercise ex1: 证明 n个顶点的图\\(G\\)，两个子图\\(G_1,G_2\\) ，任取\\(\\boldsymbol{x}\\) in \\(S_n\\)， 存在\\(\\boldsymbol{y}\\in S_n\\)，满足1. \\(f_{G_i}(\\boldsymbol{y})\\ge f_{G_i}(\\boldsymbol{x})\\) ，2. \\(\\alpha(G[supp(\\boldsymbol{y})])\\le 2\\) , (max independent number) A: 只有个大概思路，没有具体去做。假设有三个点independent，鸽巢原理，至少有两个点属于一个子图，不妨设为\\(G_1\\)，做类似上述定理的证明，证明中所构造的\\(\\boldsymbol{y&#39;}\\) 恰好同样满足在\\(G_2\\)中的不等式。 ex2: 证明一个有关一类图的邻接矩阵特征值的界。图G，\\(cl(G)=k\\) ，\\(A_G\\)邻接矩阵，有 \\(\\lambda_1(A_G)^2\\le \\frac{k-1}{k}\\left \\| A \\right \\|_F^2\\)，\\(\\lambda_1\\ge \\lambda_2\\ge \\dots\\) A: 设 \\(\\lambda_1\\) 的特征向量为 \\(\\boldsymbol{y}\\; ,\\left \\| \\boldsymbol{y} \\right \\|=1\\) ，注意：\\(\\sum_{i=1}^n y_i^2=1\\)，\\(\\left \\| A \\right \\|_F^2=2e(G)\\), \\(\\lambda_1^2=f_{G_A}^2(\\boldsymbol{y})=(\\sum y_iy_j)^2\\le 2e\\sum y_i^2y_j^2\\le \\frac{k-1}{k}\\left \\| A \\right \\|_F^2\\) Erdos-Simonovits-Stone \\(\\forall H\\) \\[ ex(n,H)=\\left ( 1-\\frac{1}{\\chi(H)-1}+o(1) \\right )\\frac{n^2}{2} \\]"},{"title":"VHDL语言入门笔记","date":"2021-03-04T05:47:19.000Z","url":"/2021/03/04/VHDL%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/","tags":[["note","/tags/note/"],["VHDL","/tags/VHDL/"]],"categories":[["Programming language","/categories/Programming-language/"],["Computer Organization","/categories/Computer-Organization/"]],"content":"at the beginningthe differences between VHDL and software programming languages Serial vs. Parallelconcurrency: ​ VHDL: all the codes are execute at the same time. =&gt; parallel language notesbasiclibrary entity architecture a simple and_gate example saved as example_and.vhd to generate a symbol of example_and , then insert it in the diagram file File -&gt; create/update -&gt; create symbol files … processoften used for sequential logic (require a clock to operate) not common usage for combinational logic (do not require a clock) example of flip-flop saved as test.vhd componentcomponent 可以将某个完成的组件作为当前系统的一个子系统。需要在architecture内，begin前声明，在begin内具体化一个实例，同时定义好component的port所对应的signal，也就是port map。另外，各种design file都可以引用来作为一个component，Quartus提供了Create VHDL Component Declaration File功能，生成一个.cmp文件，内容是自动生成的对应component声明语句，非常方便。 example of component 参考 附件计算机组成课程设计实验 实验一(目前完成的部分)click to download exp1"},{"title":"Logistic Regression and Maximum Entropy Model","date":"2021-02-15T07:56:55.000Z","url":"/2021/02/15/Logistic-regression/","tags":[["Study","/tags/Study/"],["Basic","/tags/Basic/"]],"categories":[["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"]],"content":"Logistic Regression简介：一种简单的统计分类方法，因为使用了Logistic函数得名。称作回归(Regression)的原因是，这个模型实质是在做函数的拟合问题。 Logistic Function f(x)=\\frac{1}{1+e^{-x}}介于$0$ ~ $1$之间 二分类的Logistic模型 P(Y=1|x)=\\frac{exp(w\\cdot x+b)}{1+exp(w\\cdot x+b)} P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x+b)}$w，x \\in \\mathbf{R}^n$ , $b \\in \\mathbf{R}$ 将 $b$ 并入到 $w$ 中，扩充 $w$ 和 $x$ ，从而得到更简洁的表达 P(Y=1|x)=\\frac{exp(w\\cdot x)}{1+exp(w\\cdot x)} P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x)}对模型的理解几率(odd)记事件发生的概率为 $p$ ，那么事件发生的几率为 $\\frac{p}{1-p}$ 对数几率(log odd / logit 函数) logit(p)=log\\frac{p}{1-p}对于Logistic回归来说 log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x可以看出，这是在用线性模型来拟合logit函数，故称为“回归模型” Logistic函数另一个角度看，Logistic函数将原本取值在实数集上的变量投影到了$(0,1)$ ，也就是一个概率所属的范围。 模型的求解极大似然估计，采用数值分析的方法(梯度下降法，拟牛顿法等)求解参数向量 $w$ 多分类的Logistic回归模型$Y$ 的取值范围是{1, 2, 3, … , K} P(Y=k|x)=\\frac{exp(w_k \\cdot x)}{1+\\sum_{k=1}^{K-1}exp(w_k\\cdot x)},\\quad k=1,2,\\dots, K-1 P(Y=K|x)=\\frac{1}{1+\\sum_{k=1}^{K-1}exp(w_k\\cdot x)}最大熵模型简介：最大熵模型是根据最大熵原理得到的，简单说，在所有可能的概率模型中，熵最大的模型是最好的，因为它保留了最大的不确定性，所有不确定的部分都是接近”等可能的”，减小了对不确定因素的偏见。 模型的定义利用最大熵原理，获得一个简单的分类模型。这里所求的模型是条件概率模型。根据给定的训练集，可以获得联合分布$P(X,Y)$的经验分布，以及边缘分布$P(X)$的经验分布，分别记为$\\tilde{P}(X,Y)$ 和 $\\tilde{P}(X)$ 。 约束约束就是分类问题中确定的条件，也就是输入与输出之间一些已知的事实。 约束用特征函数表示。 f(x,y)=\\left\\{\\begin{matrix} 1 ,& x和y满足某一事实\\\\ 0 ,& 否则 \\end{matrix}\\right.如果模型能够学习到数据中的信息，那么就可以假设特征函数关于经验分布和模型分布的期望是相等的。 关于经验分布的期望 E_{\\tilde{P}}(f)=\\sum_{x,y}\\tilde{P}(x,y)f(x,y)关于模型的条件分布的期望 E_P(f)=\\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y)两者应当相等 ，即 E_{\\tilde{P}}(f)=E_P(f)另外，特征函数也可不只一个。 熵条件熵 H(P)=-\\sum_{x,y}\\tilde{P}(x)P(y\\,|\\,x)\\,logP(y\\,|\\,x)模型学习约束最优化问题 \\max_{P}\\quad H(P)=-\\sum_{x,y}\\tilde{P}(x)P(y\\,|\\,x)\\,logP(y\\,|\\,x) s.t.\\quad E_{\\tilde{P}}(f_i)=E_P(f_i),\\ i=1,2,...,n \\sum_yP(y\\,|\\,x)=1根据拉格朗日乘子法， L(P,w)=-H(P)+w_0\\left ( 1-\\sum_yP(y\\,|\\,x)\\right)+\\sum_{i=1}^{n}w_i(E_{\\tilde{P}}(f_i)-E_P(f_i))问题转化为 \\min_{P}\\max_{w}L(P,w)对偶问题 \\max_{w}\\min_{P}L(P,w)因为$L(P,w)$ 是 P 的凸函数，故原始问题和对偶问题是等价的。(但其实两者等价的条件我还没学。。。我不懂，我不会) 通过对偶问题里面的极小化，可以获得一个含参的分布 P_w(y\\,|\\,x)=\\frac{1}{Z_w(x)}exp\\left ( \\sum_{i=1}^{n}w_if_i(x,y) \\right ) Z_w(x)=\\sum_yexp\\left ( \\sum_{i=1}^{n}w_if_i(x,y) \\right )其中$Z_w(x)$ 是正规化因子，$w_i$是参数、权值。 下面在进一步去求外面的极大化就可以把参数求出来。 PS：最后一步的极大化其实等价于直接利用得到的含参的分布去做极大似然估计。 模型求解 有一个改进的迭代尺度法，计算每次参数 $w$ 的增量 $\\delta$ , 关注$L(w+\\delta)-L(w)$ , 经过放缩确定下界， 为了得到更大的该变量。放缩是因为，原式太复杂。 拟牛顿法 "},{"title":"凸函数","date":"2021-01-27T13:25:10.000Z","url":"/2021/01/27/%E5%87%B8%E5%87%BD%E6%95%B0/","tags":[["Study","/tags/Study/"],["Basic","/tags/Basic/"]],"categories":[["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"],["Optimization","/categories/Optimization/"]],"content":"参考书 Boyd Convex Optimization 基本的概念和性质什么是凸函数$f:\\mathbf {R}^n \\rightarrow \\mathbf {R}, \\quad \\mathbf {dom} f$ 是个凸集，$0 \\leq \\theta \\leq 1$ f(\\theta x + (1-\\theta)y)\\leq \\theta f(x) + (1-\\theta)f(y)严格凸就是不会取到等号 凹函数就是凸函数加个负号 仿射函数是比较特殊的，它既是凸函数又是凹函数，反过来也成立。相当于凹函数和凸函数的一个共有的临界的函数。 扩展值延伸为了让凸函数在整个实数集上有定义，在定义域外令值为无穷，具体的为正无穷$+\\infty$(相反，若为凹函数，则定义为负无穷$-\\infty$) \\tilde{f}(x)=\\left\\{\\begin{matrix} f(x) &x\\in\\mathbf{dom}f\\\\ \\infty & x\\notin \\mathbf{dom}f \\end{matrix}\\right.判定条件一阶条件假设 $f$ 可微，对 $\\forall x,y \\in \\mathbf{dom}f$ f(y) \\geqslant f(x) + \\bigtriangledown f(x)^T(y-x)联系了局部信息和全局信息 严格凸及凹的情况略 二阶条件假设二阶可微，其Hessian矩阵是半正定的 未完待续。。。。"},{"title":"FFT","date":"2020-12-02T14:50:35.000Z","url":"/2020/12/02/FFT/","tags":[["Basic","/tags/Basic/"]],"categories":[[" ",""]],"content":"note : 本文仅简要地介绍FFT以及它的简单应用, 并不会过多的进行数学推导 Background首先我们回忆一下傅里叶级数. 实数上的情形对于一个函数$f(x)$, 我们现在关注它在$[-\\pi,\\pi]$ 这个区间上的表现. 它能够写成(在一定条件下) f(x) = \\frac{a_0}{2} + \\sum_{k=1}^{\\infty} a_kcos(kx)+b_ksin(kx)这里 \\begin{align*} a_k = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x)cos(kx)dx \\\\\\\\ b_k = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x)sin(kx)dx \\end{align*}虽然这里我们考虑的是在区间$[-\\pi,\\pi]$ 上, 但现在 $f$ 现在在整个$\\textbf{R}$ 具有了周期性, 周期是$2\\pi$. 另一方面, $f(x)$ 要满足”平方可积”, 详细来说, 所有平方可积的函数构成一个线性空间. 更进一步的有关希尔伯特空间等泛函分析的内容. 那这个线性空间自然具有一个无限维的基. 在傅里叶级数中, 选择的基是 \\\\{ 1, cos(x), sin(x), cos(2x), sin(2x), \\dots \\\\}在确定内积 \\left \\langle f,g\\right \\rangle = \\int_{-\\pi}^{\\pi}f(x)\\overline{g(x)}dx后,这组由 $cos(kx),sin(kx)\\dots$ 构成的基成为了一组正交基. 当然你可以选择规范化, 去除以每个的范数. 复数上的情形把实数上的情形推广到复数上, 这里是说$f(x)$ 现在是复数函数, (x依然是实数). 我们有了更一般一点的傅里叶级数 f(x) = \\sum_{k=-\\infty}^{+\\infty} c_k e^{ikx}这里 c_k = \\frac{\\left \\langle f(x), e^{ikx} \\right\\rangle}{\\left \\|e^{ikx} \\right \\|^2} = \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} f(x)e^{-ikx}dx(注: Euler公式 $e^{ix}=cos(x) + i sin(x) ,\\quad i=\\sqrt{-1}$) 仍然是用许多的$cos(kx), sin(kx)$去表达$f(x)$, 只不过变成了复数. 前面一直讲的是$f(x)$在$[-\\pi,\\pi]$上的一些事情, 实际上这并不是唯一的. 现在换成区间$[-L,L]$ 做个映射, $x \\to \\pi x/L$ 记 \\psi_k(x)=e^{i\\pi kx/L}有 c_k = \\frac{\\left \\langle f(x), \\psi_k(x) \\right\\rangle}{\\left \\|\\psi_k(x) \\right \\|^2} = \\frac{1}{2L}\\int_{-L}^{L} f(x)\\psi_k(x)dx所以 f(x) = \\sum_{k=-\\infty}^{\\infty}c_k\\psi_k(x)=\\frac{1}{2L}\\sum_{k=-\\infty}^{\\infty}\\left \\langle f(x), \\psi_k(x) \\right\\rangle \\psi_k(x)这里观察这个级数, 它实际是在将$f(x)$ 分解成一系列$\\psi_k(x)$ 的线性组合. 或者说一系列$cos(2\\pi kx/L), sin(2\\pi kx/L)$ 的结合. 这里面$k$ 蕴含着不同的正弦函数的频率. 记 \\omega_k = \\pi k/L \\\\\\\\ \\Delta \\omega = \\pi/L那么 \\begin{align*} c_k = \\frac{1}{2L} \\int_{-L}^{L} f(x)& e^{-ik{\\Delta\\omega}x}dx = \\frac{1}{2L}\\int_{-L}^{L} f(x)e^{-i\\omega_kx}dx \\\\\\\\ f(x)=& \\sum_{k=-\\infty}^{+\\infty}c_k e^{i\\omega_k x}=\\sum_{k=-\\infty}^{+\\infty}c_k e^{ik\\Delta \\omega x} \\end{align*}每个$c_k$都对应着一个$\\omega_k$, 表示这个频率的函数所占的比重. 当$L \\to +\\infty$, 也就是说$f(x)$ 在$\\textbf{R}$ 上非周期. 我们就得到了傅里叶变换. \\begin{align*} \\hat f(\\omega) & = \\mathcal{F(f(x))}=\\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}dx\\\\\\\\ f(x) =& \\mathcal{F^{-1}(\\hat f(\\omega))} = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\hat f(\\omega)e^{i\\omega x}d\\omega \\end{align*}有趣的性质: \\begin{align*} & \\mathcal{F(\\frac{d}{dx}f(x)) = i\\omega F(f(x))}\\\\\\\\\\\\\\\\ & \\mathcal{F(g\\times f) = F(g)F(f)} \\end{align*}DFT对于计算机, 我们既不能方便的处理无穷级数, 也不好处理积分. 连续的情况对计算机来说比较困难. 所以出现了离散的傅里叶变换. 他的大致含义还是一样的. 对一个$f(x)$进行采样, 在0, 1, 2, … , n - 1处的值, 分别为$[f0, f_1,f_2,…,f{n-1}]$ , 这个向量代替了原来的函数 经过傅里叶变换后的函数变成了 $[\\hat f0, \\hat f_1, \\hat f_2…\\hat f{n-1}]$ 对于 $\\omega$ 的选取也有原先连续的情况变成离散. 将 $2\\pi$ 平均分成了 n 份. 每一份长度是 $\\frac{2\\pi}{n}$ 作为一个基本的单元. 之前的函数 $\\psi_k(x)$ 成为了 $[(e^{2\\pi ik/n})^j], j=0,1,2,…,n-1$ 简单来说, DFT的形式就是 \\begin{align*} \\hat f_k=\\sum_{j=0}^{n-1}f_j e^{-2\\pi ijk/n}\\\\\\\\ f_k = \\frac{1}{n}\\sum_{j=0}^{n-1}\\hat f_k e^{2\\pi ijk/n} \\end{align*}注意 $e^{2\\pi ik/n},\\quad k=0,1,2…,n-1$ 就是 $x^{n}=1$ 的解 $\\psi_k = \\left[ \\begin{matrix} 1 \\\\ e^{2\\pi ik/n} \\\\ (e^{2\\pi ik/n})^2 \\\\ \\vdots \\\\ (e^{2\\pi ik/n})^{n-1} \\end{matrix} \\right]$ 可以看作是不同频率的函数, 那么$\\hat f_k$ 便可以看作是对应的$\\psi_k$ 所占的大小. 首先举个简单的例子去直观的理解这个变换. 我们在函数$f(x)=cos(2\\pi \\times 50 x)+cos(2\\pi \\times 75x)$ 上进行采样, 同时我们加上一个随机的噪声,范围是$[-4,4]$ 黑色的是原图像, 蓝色的部分是加入噪声后的图象. 当然这里的数据都是离散的, 只不过画图的时候被连在了一起.看上去像连续的. 现在我们得到了采样后的一个$f(x)$的向量, $[f0,f_1,f_2,…,f{n-1}]$ 经过DFT后, 我们能得到一个$\\hat f$ 向量, 由于是复数, 这里只展示向量中每个复数的模长的平方. 可以看作是一种能量. 下面的图是经过DFT后的样子, 明显可以看到, 里面有两个非常高的点. 对应着原始的没有加入噪声的$cos(2\\pi \\times 50 x)$ 和 $cos(2\\pi \\times 75x)$ , 如果我们设置一个阈值, 小于100的置位0, 这样就能去除噪声. 中间的图就是对$\\hat f$ 进行过滤之后再经过DFT的逆操作的得到的图象, 会发现它完全去除了噪声, 还原了最初的$f(x)$ . 计算DFT下面的问题是如何去计算DFT呢, \\begin{align*} \\hat f_k=\\sum_{j=0}^{n-1}f_j e^{-2\\pi ijk/n}\\\\\\\\ f_k = \\frac{1}{n}\\sum_{j=0}^{n-1}\\hat f_k e^{2\\pi ijk/n} \\end{align*}容易看到这实质上是一个矩阵乘法. 记 $\\omega_n = e^{-2\\pi i/n}$ \\hat f=\\left [ \\begin{matrix} \\hat f_0 \\\\\\\\ \\hat f_1 \\\\\\\\ \\hat f_2 \\\\\\\\ \\vdots \\\\\\\\ \\hat f_{n-1} \\end{matrix} \\right ] = \\left [ \\begin{matrix} 1 & 1 & 1 & 1 & \\dots & 1 \\\\\\\\ 1 & \\omega_n & \\omega_n^{2} & \\omega_n^{3} & \\dots & \\omega_n^{n-1}\\\\\\\\ 1 & \\omega_n^{2} & \\omega_n^{4} & \\omega_n^{6} & \\dots & \\omega_n^{2(n-1)} \\\\\\\\ & & \\vdots & & \\dots &\\vdots \\\\\\\\ 1 & \\omega_n^{n-1} & \\omega_n^{2(n-1)} & \\omega_n^{3(n-1)} & \\dots & \\omega_n^{(n-1)(n-1)} \\end{matrix}\\right] \\left[ \\begin{matrix} f_0 \\\\\\\\ f_1 \\\\\\\\ f_2 \\\\\\\\ \\vdots \\\\\\\\ f_{n-1} \\end{matrix}\\right]记这个矩阵为$\\mathcal{F_n}$ , 或者称为DFT矩阵. 实质上$\\mathcal{F}$ 乘上它的共轭转置 $\\mathcal{F^*}$ 等于 $nE_n$， 所以它的逆矩阵是很好求的，DFT的逆变换也很容易得出. 如果直接去求这个矩阵，复杂度是比较高的$O(n^2)$ 但是如果考虑到 $\\omega_n$ 的特殊的性质，便能得到非常高效的$O(nlogn)$ 的算法，也就是FFT FFTFFT就是一种计算DFT的高效的算法，它最初是由高斯提出. 简单来说，我们想要快速的计算矩阵$\\mathcal{Fn}$ ，通过将他分解为更小的子问题，转而去求解$\\mathcal{F{\\frac{n}{2}}}$. 这里我们现在只考虑 n 是2的幂的情况. 如果n不是2的幂的话， 我们可以简单的扩充成2的幂， 并不会影响算法的效率. 不同的矩阵$\\mathcal{Fn}$ 内所使用的$\\omega_n$ 是不同的， 但对于n 和 n/2 我们有 $\\omega_n^2=\\omega{n/2}$ 可以轻松的进行转化。 核心的想法就是对$[f0, f_1,…,f{n-1}]$ 进行重新排序， 按照下标的奇偶进行分组. 对于多项式 \\begin{align*} p(x) =& a_0+a_1x+a_2x^2+\\dots+a_{n-1}x^{n-1} \\\\\\\\ =& (a_0 + a_2x^2 + a_4x^4 + \\dots + a_{n-2}x^{n-2})\\\\\\\\ &+ x(a_1 + a_3x^2+a_5x^4+\\dots a_{n-1}x^{n-2})\\\\\\\\ =& E(x) +xO(x) \\end{align*}同理，对于这里的DFT， 我们就会的到 \\begin{align*} \\hat f_k =& \\sum_{j=0}^{n-1}f_j(\\omega_n^k)^j\\\\\\\\ =& \\sum_{j=0}^{n/2-1}f_{2j}(\\omega_n^k)^{2j} +\\omega_n^k\\sum_{j=0}^{n/2-1}f_{2j+1}(\\omega_n^k)^{2j}\\\\\\\\ =& \\sum_{j=0}^{n/2-1}f_{2j}(\\omega_{n/2}^k)^{j} +\\omega_n^k\\sum_{j=0}^{n/2-1}f_{2j+1}(\\omega_{n/2}^k)^{j}\\\\\\\\ =& E + \\omega_n^k O \\end{align*}$E$ 和 $O$ 便是我们的两个相同的子问题. 如果我们分别利用 $f{even}$ 和 $f{odd}$ 计算出了$E$ 和 $O$ 我们便能很快的根据上面的式子计算出所有的$\\hat f_k$ 不仅如此，我们可以利用 \\omega_n^{k+n/2} = -\\omega_n^{k}进一步的简化. \\begin{align*} \\hat f_k=E+\\omega_n^kO\\\\\\\\ \\hat f_{k+n/2}=E-\\omega_n^kO \\end{align*}这样只需要一半的循环，就能计算出全部的$\\hat f_k$ \\begin{align*} \\hat f=\\left [ \\begin{matrix} \\hat f_0 \\\\\\\\ \\hat f_1 \\\\\\\\ \\hat f_2 \\\\\\\\ \\vdots \\\\\\\\ \\hat f_{n-1} \\end{matrix} \\right ] =& \\mathcal{F_n} \\left[ \\begin{matrix} f_0 \\\\\\\\ f_1 \\\\\\\\ f_2 \\\\\\\\ \\vdots \\\\\\\\ f_{n-1} \\end{matrix}\\right]\\\\\\\\ =&\\left[ \\begin{matrix} E_{n/2} & D_{n/2} \\\\\\\\\\\\\\\\ E_{n/2} & -D_{n/2} \\end{matrix}\\right] \\left[ \\begin{matrix} \\mathcal{F_{n/2}} & O \\\\\\\\\\\\\\\\ O & \\mathcal{F_{n/2}} \\end{matrix}\\right] \\left[ \\begin{matrix} f_{even}\\\\\\\\\\\\\\\\f_{odd} \\end{matrix}\\right] \\end{align*}如此算法的复杂度达到了$O(nlogn)$ ，如果利用并行计算，可以达到更快. FFT的应用 像前面做过的去除噪声便是一个很常见的应用 图象压缩 多项式乘法或者大整数乘法 "},{"title":"Word2Vec","date":"2020-11-10T16:03:16.000Z","url":"/2020/11/11/Word2Vec/","tags":[["Machine Learning","/tags/Machine-Learning/"],["NLP","/tags/NLP/"]],"categories":[["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"],["Machine Learning","/categories/Machine-Learning/"]],"content":"beginning其实用向量来表示一个单词是很常见的, 毕竟方便一个算法或模型去表示一个单词. 但表示的方法很重要, 最完美的Embedding就是能够包含单词的语义, 相似语义的单词的表示越接近, 反之表示的向量差距越大. 下面一步步的去完成我们的目标. distance怎么来叙述两个单词的embedding是相近的呢? 容易想到对于两个向量, 我们有一个适用于 $n$ 维空间上的一个夹角公式. for $\\vec{a}$, $\\vec{b}$ as two embedding vectors of different words. the cosine similarity is Cosine Similarity(\\vec{a}, \\vec{b}) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}|\\cdot|\\vec{b}|}这个评判函数相对简单, 也非常有效. 对于不同的问题, 我们也可以选择不同的距离函数. modelingembedding都是为了更好的完成一个task, 那么下面先来看一个简单的任务, 在完成任务的同时, 来获得一个适合它的embedding. predicting next word根据语境预测下一个单词是什么. 这是一个在日常生活中非常常见的任务.简单来做, 就是分为三步, 输入一个单词, 先获得对应的embedding 利用embedding去预测下一个单词的embedding 根据embedding映射到对应的单词, 然后输出 数据集从哪来呢 一般是Wikipedia的文章等等 构造许多定长的input - output对, 做法是window sliding with a fixed length, 前面的都是input, 最后一个单词是output problem这么做有两个问题. 预测一个单词怎么能只看前几个单词呢 最后有个 $n \\times d$ , (其中 $n$ 表示Vacab的大小, $d$ 表示embedding的大小) , 这算起来太费时了 problem solving 取样时不仅取前面的, 把后面的也取了. 这叫skip gram. 这就成了Clozing了 换个方法, 把他改成预测谁是neighbor, 输出可能是neighbor的概率 computational problem咋整. 再调整我们的任务 我们改成更小规模的模型, 给他一个input, 和一堆output, 然后输出这些output分别是不是他的neighbor 那有可能我们训出了一个人工智障, 他告诉我们这些全都是neighbor, 我们好像也没法反驳. 给他来个negative sample, 告诉model他们不是neighbor, 这样model就成了个logistics regression模型, 规模小了很多 但咱采样的时候怎么知道他们到底有没有可能是neighbor, 我们手上的数据集肯定是不完备的呀 随机设为negative…… 问题基本解决了, 下面看具体流程 Word2Vec 我们训两个embeddings, 叫做Embedding和Context, 分别为input和一组outputs做project. 获得两组embedding: $\\vec{input}$ 和 $\\vec{output}$. 这个outputs肯定就是一些真正的neighbor(通过skip gram window sliding选出来的)加上一些随机的negative sample. input $\\to$ Embedding $\\to$ $\\vec{input}$ outputs $\\to$ Context $\\to$ a set of $\\vec{output}$s 然后点乘, 来个softmax, 获得这么个概率, 根据误差不断训 最后, 训得差不多了, 扔掉context, 这个Embedding就是炼出来的丹 last 语义相近的那些单词, 被认为是所在的Context是相似的. 越相似的词点乘上Context, 得到的结果肯定都很接近正确结果. 这个window sliding 的length一般设成5, 越小的话得到的embedding划的越细, 就是说相近的embedding的单词所在的context几乎一样, 但要注意, 反义词很多时候也是这样, 把length设大点就能区分更多语境 Endreference a blog 还没看过原paper和代码, 估计等以后了 "},{"title":"多边形的扫描转换与区域填充算法","date":"2020-11-09T14:33:08.000Z","url":"/2020/11/09/%E5%A4%9A%E8%BE%B9%E5%BD%A2%E7%9A%84%E6%89%AB%E6%8F%8F%E8%BD%AC%E6%8D%A2%E4%B8%8E%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85%E7%AE%97%E6%B3%95/","tags":[["图形学","/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"],["数据结构","/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"]],"categories":[["Assign","/categories/Assign/"]],"content":"多边形的扫描转换与区域填充算法为了完成这次算法作业, 需要C++ 图形编程. 我选择了OpenGL. 配置起来有点麻烦, 又遇到了一些Bug, 最后索性用了GL里面的一个库, glut.h 进行尝试. Cheating CodeGLUT的学习用的Clion, 配置了库, 但是可能有问题, 至少Glut是能正常使用的 随便找的一篇博客配置OpenGL 随便找到的一篇GLUT Tutorial 捣腾半天发现其实我需要的功能并不是很复杂, 也没用到太多glut.h更深入的东西. 了解了glut基本的结构以后还是比较容易看懂整体的流程的. main() myInit() recall to be continue… 算法代码之后再说…."},{"title":"Transformer模型学习笔记","date":"2020-11-08T17:22:25.000Z","url":"/2020/11/09/Transformer%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","tags":[["Machine Learning","/tags/Machine-Learning/"],["NLP","/tags/NLP/"]],"categories":[["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"],["Machine Learning","/categories/Machine-Learning/"]],"content":"Transformer 模型学习笔记论文地址Attention Is All You NeedNice Blog for illustrating Transformer Modelseq2seq with attention提出的特点 RNN无法并行地去处理一个序列, 因为每个hidden state $hi$都是依赖于上一个hidden state $h{i-1}$以及input. 所以就要一个step接一个step的去循环, 对于很长的序列训练起来就很耗时. Transformer 模型利用Attention机制去捕获全局的input与output之间的依赖性, 实质上就是将整条序列看作一个input向量, 也就避免了循环神经网络中的”循环“. 实质上算是对RNN循环过程的一个展开吧. 完全使用Attention机制, 没有使用序列对齐的循环(sequence-aligned recurrence)或者卷积层 背景知识Self attention的优势 Gated RNNs 虽然在结构上能够记录前 $n - 1$ 个token的信息, 但实际上, 随着序列变长, 最早的token信息会变得很少, 这就会失去他的准确性, 这在翻译任务中就显得非常要命, 例如, 在English-to-French的翻译里, output的第一个词大概率是依赖于input开始的部分, 这样很可能会得到很差的结果. 而transformer模型似乎是靠着更大的存储和算力来强行将前 $n$ 个token利用attention融合起来. 这样看来, 似乎是对症下药, 实验上也得到了很好的结果. 而具体的self attention会在模型架构中介绍 Word EmbeddingBlog for introducing Word2Vec我自己的对Word2Vec的学习笔记 模型架构其实Transformer模型的改进应该去对比seq2seq with attention模型 首先在seq2seq模型里,用的是最初的encoder-decoder思想, 拿翻译任务来说, input就是一个句子, encoder需要一个单词一个单词(token)的去encode, 也就是扔进一个RNNs, 每次得到一个hidden state, 句子全输进去了, 最后得到的hidden state就可以作为整个句子的representation. 这个思想很简单, 给我的是不定长的, 那我就把他搞成一个固定长度的hidden state. 之后拿着这个representation当作decoder的input, 再一个单词一个单词的预测. 前后都是RNN. 乍一看貌似还是挺好的, 但问题是RNN的记忆机制和梯度问题解决的不是那么好, 句子长了前面的单词他就忘记了. 而且翻译这个任务也确实需要一种attention ,output的某一个部分会很大程度依赖于input中的一部分. 加了attention的seq2seq似乎就考虑到了这种依赖关系, attention机制也很好的做到了这一点. 至于整体做法, 上面不是说encoder内不断地生成hidden state吗, 那我们就把它们全都取出来作为decoder的input, 这样就不用太担心记忆的问题了, 毕竟你把它们都拿出来了. 然后每个output预测值会利用attention机制去给这些hidden state附上注意力的权重, 来更好地完成任务. 再到transformer. 这样纵向的来看, 似乎改进的地方确实如原论文所讲, 去掉了所有的循环连接. 完全用attention来解决. 这样做就需要在一些地方进行调整. Attention虽然字面上讲的attention, 似乎是个很熟悉的概念, 但实际在具体的实现上是另一种更加抽象的机制. 一般来讲,这个问题是想输入两种序列, $S= {S1, S_2, S_3 \\dots S_n }$ 以及 $T ={T_1, T_2, T_3 \\dots T_m }$ 我想输出$T$ 对于$S$ 的attention, 具体的可以说就是一个function, $Attention{S_i}(T_j),\\quad i= 1,2\\dots n$ ,然后有 \\sum_{j} Attention_{S_i}(T_j)=1, \\quad i = 1,2,\\dots n就可以,值越大就越重要. 那么整个T对于 $Si$ 的representation就是 $T{si} = \\sum{j} Attention_{S_i} (T_j)\\times Value(T_j)$ 这么一个加权平均 这里的元素就是一些embedding, 一些向量. Attention的求法类似于一种查询. 每个 $Si$ 都会对应一个query向量 $\\boldsymbol{q_i}$ 每个 $T_i$ 又对应一个键值 $\\boldsymbol{k_i}$ 以供”查询”, 查到的结果就是两个向量的点积 $a{ij} = \\boldsymbol{q_i} \\cdot \\boldsymbol{k_j}$ , (假设这里的两个向量的维数都是 $d_k$ ). 最后的Attention就是再加上一个softmax Attention_{S_i}(T_j) = \\frac{e^{\\boldsymbol{q_i} \\cdot \\boldsymbol{k_j}^T}}{\\sum_j e^{\\boldsymbol{q_i} \\cdot \\boldsymbol{k_j}^T}}每个 $T_j$ 又会对应一个Value向量 $v_j$ (维度可以和前面两个向量不同, 记为$d_v$)用以获得representation. 最后得到的就是 Representation_{for\\ S_i} = softmax(\\boldsymbol{q}_{1\\times d_k}\\cdot \\boldsymbol K_{m \\times d_k}^T) \\boldsymbol V_{m\\times d_k}进一步可以获得 $T$ 对 $S$ 的表示 softmax(\\boldsymbol{Q}_{n\\times d_k}\\cdot \\boldsymbol K_{m \\times d_k}^T) \\boldsymbol V_{m\\times d_v}为了”having more stable gradients” , 在$\\boldsymbol{Q}\\cdot \\boldsymbol K^T$这里还要除以一个因子, 默认是 $\\sqrt{d_k}$ , 然后又变成了 softmax(\\frac{\\boldsymbol{Q}\\cdot \\boldsymbol K^T}{\\sqrt{d_k}}) \\boldsymbol V有人要问了, 你说的这些query,key和value向量都咋求呢. 用三个线性映射(矩阵) $W^Q,W^K,W^V$ 线性映射哪来的呢 学出来的 a beast with multihead这样一组attention可能注意力太集中, 看不全, 那我们就让他有多个”头”, 注意力分散点, 看得更全 tensor2tensor上有个示例, 演示的就是attention 一个比较经典的例子 翻译句子 The animal didn’t cross the street because it was too tired 这里的 it 应该代指 The animal, 但是对模型来说, 他也可能是说the street. 这里可以看到, 模型确实被it的代指给弄晕了,但还好animal处的颜色比street的地方要深,说明他的权值要大 但并不是所有的attention都能学到对应的部分. 解决办法就是, 我们用多个attention去拼接成最终想要的representation. 具体的, 我们得到的value向量不是 $dv$ 维的吗, 假设我们有 $h$ 个head, 那么就把向量分为$h$ 个维度为 $d_v / h$ 的向量, 每个用各自的线性映射得到 $h$ 组不同的 $\\boldsymbol{Q,\\, K,\\, V{m \\times (d_v/h)}}$ 去求各自的value(attention结构图片来自这个blog) 最后一般还会再乘上一个矩阵 $W^O$,来得到最后的输出 用上了多个head, 我们就能同时去关注不同的区域, 获得更准确的表述 整体架构前面花了较长篇幅讲了Attention机制, 这里再看下它是如何被用在Transformer中的 在上面的架构图中包含encoder以及decoder的结构(结构图来自原论文) 左边是encoder, 他的特点就是直接将整条序列直接放进网络层中. 工作流程就是, 首先把要处理的序列input输入, 再获得它的embeddings. 然后依次进入每个encoder层, "},{"title":"我的第一篇文章","date":"2020-11-08T11:03:14.000Z","url":"/2020/11/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/","tags":[["test","/tags/test/"]],"categories":[["test","/categories/test/"]],"content":"this is my first blog here"},{"title":"about","date":"2020-11-08T12:32:12.000Z","url":"/about/index.html","categories":[[" ",""]],"content":"Wu Shiguang.Taishan College, Shandong University"},{"title":"categories","date":"2020-11-08T17:39:56.000Z","url":"/categories/index.html","categories":[[" ",""]]},{"title":"search","date":"2020-11-08T12:14:33.000Z","url":"/search/index.html","categories":[[" ",""]]},{"title":"tags","date":"2020-11-08T12:30:26.000Z","url":"/tags/index.html","categories":[[" ",""]]}]