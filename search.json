[{"title":"Word2Vec","date":"2020-11-10T16:03:16.000Z","url":"/2020/11/11/Word2Vec/","tags":[["Machine Learning","/tags/Machine-Learning/"],["NLP","/tags/NLP/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"]],"content":"beginning其实用向量来表示一个单词是很常见的, 毕竟方便一个算法或模型去表示一个单词. 但表示的方法很重要, 最完美的Embedding就是能够包含单词的语义, 相似语义的单词的表示越接近, 反之表示的向量差距越大. 下面一步步的去完成我们的目标. distance怎么来叙述两个单词的embedding是相近的呢? 容易想到对于两个向量, 我们有一个适用于 $n$ 维空间上的一个夹角公式. for $\\vec{a}$, $\\vec{b}$ as two embedding vectors of different words. the cosine similarity is$$Cosine Similarity(\\vec{a}, \\vec{b}) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}|\\cdot|\\vec{b}|}$$ 这个评判函数相对简单, 也非常有效. 对于不同的问题, 我们也可以选择不同的距离函数. modelingembedding都是为了更好的完成一个task, 那么下面先来看一个简单的任务, 在完成任务的同时, 来获得一个适合它的embedding. predicting next word根据语境预测下一个单词是什么. 这是一个在日常生活中非常常见的任务.简单来做, 就是分为三步, 输入一个单词, 先获得对应的embedding 利用embedding去预测下一个单词的embedding 根据embedding映射到对应的单词, 然后输出 数据集从哪来呢 一般是Wikipedia的文章等等 构造许多定长的input - output对, 做法是window sliding with a fixed length, 前面的都是input, 最后一个单词是output problem这么做有两个问题. 预测一个单词怎么能只看前几个单词呢 最后有个 $n \\times d$ , (其中 $n$ 表示Vacab的大小, $d$ 表示embedding的大小) , 这算起来太费时了 problem solving 取样时不仅取前面的, 把后面的也取了. 这叫skip gram. 这就成了Clozing了 换个方法, 把他改成预测谁是neighbor, 输出可能是neighbor的概率 computational problem咋整. 再调整我们的任务 我们改成更小规模的模型, 给他一个input, 和一堆output, 然后输出这些output分别是不是他的neighbor 那有可能我们训出了一个人工智障, 他告诉我们这些全都是neighbor, 我们好像也没法反驳. 给他来个negative sample, 告诉model他们不是neighbor, 这样model就成了个logistics regression模型, 规模小了很多 但咱采样的时候怎么知道他们到底有没有可能是neighbor, 我们手上的数据集肯定是不完备的呀 随机设为negative…… 问题基本解决了, 下面看具体流程 Word2Vec 我们训两个embeddings, 叫做Embedding和Context, 分别为input和一组outputs做project. 获得两组embedding: $\\vec{input}$ 和 $\\vec{output}$. 这个outputs肯定就是一些真正的neighbor(通过skip gram window sliding选出来的)加上一些随机的negative sample. input $\\to$ Embedding $\\to$ $\\vec{input}$ outputs $\\to$ Context $\\to$ a set of $\\vec{output}$s 然后点乘, 来个softmax, 获得这么个概率, 根据误差不断训 最后, 训得差不多了, 扔掉context, 这个Embedding就是炼出来的丹 last 语义相近的那些单词, 被认为是所在的Context是相似的. 越相似的词点乘上Context, 得到的结果肯定都很接近正确结果. 这个window sliding 的length一般设成5, 越小的话得到的embedding划的越细, 就是说相近的embedding的单词所在的context几乎一样, 但要注意, 反义词很多时候也是这样, 把length设大点就能区分更多语境 Endreference a blog 还没看过原paper和代码, 估计等以后了 "},{"title":"多边形的扫描转换与区域填充算法","date":"2020-11-09T14:33:08.000Z","url":"/2020/11/09/%E5%A4%9A%E8%BE%B9%E5%BD%A2%E7%9A%84%E6%89%AB%E6%8F%8F%E8%BD%AC%E6%8D%A2%E4%B8%8E%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85%E7%AE%97%E6%B3%95/","tags":[["图形学","/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"],["数据结构","/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"]],"categories":[["Assign","/categories/Assign/"]],"content":"多边形的扫描转换与区域填充算法为了完成这次算法作业, 需要C++ 图形编程. 我选择了OpenGL. 配置起来有点麻烦, 又遇到了一些Bug, 最后索性用了GL里面的一个库, glut.h 进行尝试. Cheating CodeGLUT的学习用的Clion, 配置了库, 但是可能有问题, 至少Glut是能正常使用的 随便找的一篇博客配置OpenGL 随便找到的一篇GLUT Tutorial TODO (demo: 用鼠标选点, [ENTER] 输入选好的点, 然后连线, 输出点的坐标)"},{"title":"Transformer模型学习笔记","date":"2020-11-08T17:22:25.000Z","url":"/2020/11/09/Transformer%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","tags":[["Machine Learning","/tags/Machine-Learning/"],["NLP","/tags/NLP/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"]],"content":"Transformer 模型学习笔记论文地址Attention Is All You NeedNice Blog for illustrating Transformer Modelseq2seq with attention提出的特点 RNN无法并行地去处理一个序列, 因为每个hidden state $h_i$都是依赖于上一个hidden state $h_{i-1}$以及input. 所以就要一个step接一个step的去循环, 对于很长的序列训练起来就很耗时. Transformer 模型利用Attention机制去捕获全局的input与output之间的依赖性, 实质上就是将整条序列看作一个input向量, 也就避免了循环神经网络中的”循环“. 实质上算是对RNN循环过程的一个展开吧. 完全使用Attention机制, 没有使用序列对齐的循环(sequence-aligned recurrence)或者卷积层 背景知识Self attention的优势 Gated RNNs 虽然在结构上能够记录前 $n - 1$ 个token的信息, 但实际上, 随着序列变长, 最早的token信息会变得很少, 这就会失去他的准确性, 这在翻译任务中就显得非常要命, 例如, 在English-to-French的翻译里, output的第一个词大概率是依赖于input开始的部分, 这样很可能会得到很差的结果. 而transformer模型似乎是靠着更大的存储和算力来强行将前 $n$ 个token利用attention融合起来. 这样看来, 似乎是对症下药, 实验上也得到了很好的结果. 而具体的self attention会在模型架构中介绍 Word EmbeddingBlog for introducing Word2Vec TODO 学习Word2Vec 模型架构 在上面的架构图中包含encoder以及decoder的结构transformer model完成的翻译任务实际上还是encoder-decoder的一个结构, TODO (与seq2seq with attention 有很多重合的部分, 找出核心的不同, 理解output(shift)为啥能作为init decoder input, 和怎么mask)"},{"title":"我的第一篇文章","date":"2020-11-08T11:03:14.000Z","url":"/2020/11/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/","tags":[["test","/tags/test/"]],"categories":[["test","/categories/test/"]],"content":"this is my first blog here"},{"title":"Hello World","date":"2020-11-08T10:48:32.418Z","url":"/2020/11/08/hello-world/","categories":[[" ",""]],"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post More info: Writing Run server More info: Server Generate static files More info: Generating Deploy to remote sites More info: Deployment"},{"title":"search","date":"2020-11-08T12:14:33.000Z","url":"/search/index.html","categories":[[" ",""]]},{"title":"about","date":"2020-11-08T12:32:12.000Z","url":"/about/index.html","categories":[[" ",""]],"content":"Wu Shiguang.Taishan College, Shandong University"},{"title":"tags","date":"2020-11-08T12:30:26.000Z","url":"/tags/index.html","categories":[[" ",""]]},{"title":"categories","date":"2020-11-08T17:39:56.000Z","url":"/categories/index.html","categories":[[" ",""]]}]