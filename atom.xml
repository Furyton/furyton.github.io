<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Furyton</title>
  
  <subtitle>here is my channel</subtitle>
  <link href="http://furyton.github.io/atom.xml" rel="self"/>
  
  <link href="http://furyton.github.io/"/>
  <updated>2021-03-05T14:31:08.913Z</updated>
  <id>http://furyton.github.io/</id>
  
  <author>
    <name>Wu Shiguang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>VHDL语言入门笔记</title>
    <link href="http://furyton.github.io/2021/03/04/VHDL%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/"/>
    <id>http://furyton.github.io/2021/03/04/VHDL%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/</id>
    <published>2021-03-04T05:47:19.000Z</published>
    <updated>2021-03-05T14:31:08.913Z</updated>
    
    <content type="html"><![CDATA[<h1 id="at-the-beginning">at the beginning</h1><p>the differences between VHDL and software programming languages</p><h2 id="serial-vs.-parallel">Serial vs. Parallel</h2><p><strong>concurrency</strong>:</p><p>​ VHDL: all the codes are execute at the <strong>same</strong> time. =&gt; parallel language</p><h1 id="notes">notes</h1><h3 id="basic">basic</h3><h4 id="library">library</h4><pre><code class="hljs vhdl"><span class="hljs-keyword">library</span> ieee;<span class="hljs-keyword">use</span> ieee.std_logic_1164.<span class="hljs-keyword">all</span>;....</code></pre><h4 id="entity">entity</h4><pre><code class="hljs vhdl"><span class="hljs-keyword">entity</span> name_same_with_filename <span class="hljs-keyword">is</span><span class="hljs-keyword">port</span> ( <span class="hljs-comment">-- the ports opened for the users</span>input_1: <span class="hljs-keyword">in</span> <span class="hljs-built_in">std_logic</span>; <span class="hljs-comment">-- std_logic have more than states &#x27;1&#x27; and &#x27;0&#x27;</span>        ...        result: <span class="hljs-keyword">out</span> <span class="hljs-built_in">std_logic</span> <span class="hljs-comment">-- PortName : &lt;mode&gt; &lt;type&gt;</span>        <span class="hljs-comment">-- the last port is not followed by a &#x27;;&#x27;</span>);<span class="hljs-keyword">end</span> <span class="hljs-keyword">entity</span>; <span class="hljs-comment">-- remember the &#x27;;&#x27;</span></code></pre><h4 id="architecture">architecture</h4><pre><code class="hljs vhdl"><span class="hljs-keyword">architecture</span> arch_name <span class="hljs-keyword">of</span> entity_name <span class="hljs-keyword">is</span>    <span class="hljs-comment">-- signal declarations(used for internal connections)</span>   <span class="hljs-keyword">signal</span> name: <span class="hljs-built_in">bit</span>;    <span class="hljs-comment">-- constant</span><span class="hljs-keyword">begin</span>    <span class="hljs-comment">-- behavioral of the system</span><span class="hljs-keyword">end</span> <span class="hljs-keyword">architecture</span>;</code></pre><h4 id="a-simple-and_gate-example">a simple and_gate example</h4><pre><code class="hljs vhdl"><span class="hljs-keyword">library</span> ieee;<span class="hljs-keyword">use</span> ieee.std_logic_1164.<span class="hljs-keyword">all</span>; <span class="hljs-keyword">entity</span> example_and <span class="hljs-keyword">is</span>  <span class="hljs-keyword">port</span> (    input_1    : <span class="hljs-keyword">in</span>  <span class="hljs-built_in">std_logic</span>;    input_2    : <span class="hljs-keyword">in</span>  <span class="hljs-built_in">std_logic</span>;    and_result : <span class="hljs-keyword">out</span> <span class="hljs-built_in">std_logic</span>    );<span class="hljs-keyword">end</span> example_and; <span class="hljs-keyword">architecture</span> rtl <span class="hljs-keyword">of</span> example_and <span class="hljs-keyword">is</span>  <span class="hljs-keyword">signal</span> and_gate : <span class="hljs-built_in">std_logic</span>;<span class="hljs-keyword">begin</span>  and_gate   &lt;= input_1 <span class="hljs-keyword">and</span> input_2;  and_result &lt;= and_gate;<span class="hljs-keyword">end</span> rtl;</code></pre><p>saved as example_and.vhd</p><p>to generate a symbol of example_and , then insert it in the diagram file</p><ul><li>File -&gt; create/update -&gt; create symbol files ...</li></ul><h3 id="process">process</h3><p>often used for sequential logic (require a clock to operate)</p><p>not common usage for combinational logic (do not require a clock)</p><h4 id="example-of-flip-flop">example of flip-flop</h4><pre><code class="hljs vhdl"><span class="hljs-keyword">library</span> ieee;<span class="hljs-keyword">use</span> ieee.std_logic_1164.<span class="hljs-keyword">all</span>;<span class="hljs-keyword">entity</span> test <span class="hljs-keyword">is</span><span class="hljs-keyword">port</span> (i_clock : <span class="hljs-keyword">in</span> <span class="hljs-built_in">std_logic</span>;q1, q2, q3, q4 : <span class="hljs-keyword">out</span> <span class="hljs-built_in">std_logic</span>);<span class="hljs-keyword">end</span> test;<span class="hljs-keyword">architecture</span> rtl <span class="hljs-keyword">of</span> test <span class="hljs-keyword">is</span><span class="hljs-keyword">signal</span> test1 : <span class="hljs-built_in">std_logic</span> := <span class="hljs-string">&#x27;1&#x27;</span>;<span class="hljs-keyword">signal</span> test2 : <span class="hljs-built_in">std_logic</span> := <span class="hljs-string">&#x27;0&#x27;</span>;<span class="hljs-keyword">signal</span> test3 : <span class="hljs-built_in">std_logic</span> := <span class="hljs-string">&#x27;0&#x27;</span>;<span class="hljs-keyword">signal</span> test4 : <span class="hljs-built_in">std_logic</span> := <span class="hljs-string">&#x27;0&#x27;</span>;<span class="hljs-keyword">begin</span>flip : <span class="hljs-keyword">process</span> (i_clock) <span class="hljs-keyword">is</span><span class="hljs-keyword">begin</span><span class="hljs-keyword">if</span> rising_edge(i_clock) <span class="hljs-keyword">then</span>test2 &lt;= test1;test3 &lt;= test2;test4 &lt;= test3;<span class="hljs-keyword">end</span> <span class="hljs-keyword">if</span>;<span class="hljs-keyword">end</span> <span class="hljs-keyword">process</span> flip;        <span class="hljs-comment">-- or without a name</span>                <span class="hljs-comment">-- process (i_clock)</span><span class="hljs-comment">-- begin</span><span class="hljs-comment">-- if rising_edge(i_clock) then</span><span class="hljs-comment">-- test2 &lt;= test1;</span><span class="hljs-comment">-- test3 &lt;= test2;</span><span class="hljs-comment">-- test4 &lt;= test3;</span><span class="hljs-comment">-- end if;</span><span class="hljs-comment">-- end process;</span>q1 &lt;= test1;q2 &lt;= test2;q3 &lt;= test3;q4 &lt;= test4;<span class="hljs-keyword">end</span> rtl;</code></pre><p>saved as test.vhd</p><h3 id="component">component</h3><p>component 可以将某个完成的组件作为当前系统的一个子系统。需要在architecture内，begin前声明，在begin内具体化一个实例，同时定义好component的port所对应的signal，也就是port map。另外，各种design file都可以引用来作为一个component，Quartus提供了Create VHDL Component Declaration File功能，生成一个.cmp文件，内容是自动生成的对应component声明语句，非常方便。</p><h4 id="example-of-component">example of component</h4><pre><code class="hljs vhdl"><span class="hljs-keyword">library</span> ieee;<span class="hljs-keyword">use</span> ieee.std_logic_1164.<span class="hljs-keyword">all</span>;<span class="hljs-keyword">entity</span> uPC <span class="hljs-keyword">is</span><span class="hljs-keyword">port</span>(LOAD : <span class="hljs-keyword">in</span> <span class="hljs-built_in">std_logic</span>;CPuPC  : <span class="hljs-keyword">in</span> <span class="hljs-built_in">std_logic</span>;E   : <span class="hljs-keyword">in</span> <span class="hljs-built_in">std_logic</span>;CLR  : <span class="hljs-keyword">in</span> <span class="hljs-built_in">std_logic</span>;D    : <span class="hljs-keyword">in</span> <span class="hljs-built_in">std_logic_vector</span>(<span class="hljs-number">15</span> <span class="hljs-keyword">downto</span> <span class="hljs-number">0</span>);Q    : <span class="hljs-keyword">out</span> <span class="hljs-built_in">std_logic_vector</span>(<span class="hljs-number">15</span> <span class="hljs-keyword">downto</span> <span class="hljs-number">0</span>));<span class="hljs-keyword">end</span> <span class="hljs-keyword">entity</span> uPC;<span class="hljs-keyword">architecture</span> rtl <span class="hljs-keyword">of</span> uPC <span class="hljs-keyword">is</span><span class="hljs-keyword">COMPONENT</span> f74161 <span class="hljs-comment">-- f74161是Quartus library所带的74161的bdf文件，实质上在图形界面里使用的是它的一个子部件调用，还有一个p74161，貌似是根据device family来选择两者，具体的细节不太了解。</span><span class="hljs-keyword">PORT</span> <span class="hljs-comment">-- 声明port</span>(CLRN: <span class="hljs-keyword">IN</span> <span class="hljs-built_in">STD_LOGIC</span>;LDN: <span class="hljs-keyword">IN</span> <span class="hljs-built_in">STD_LOGIC</span>;D: <span class="hljs-keyword">IN</span> <span class="hljs-built_in">STD_LOGIC</span>;C: <span class="hljs-keyword">IN</span> <span class="hljs-built_in">STD_LOGIC</span>;B: <span class="hljs-keyword">IN</span> <span class="hljs-built_in">STD_LOGIC</span>;ENP: <span class="hljs-keyword">IN</span> <span class="hljs-built_in">STD_LOGIC</span>;A: <span class="hljs-keyword">IN</span> <span class="hljs-built_in">STD_LOGIC</span>;ENT: <span class="hljs-keyword">IN</span> <span class="hljs-built_in">STD_LOGIC</span>;CLK: <span class="hljs-keyword">IN</span> <span class="hljs-built_in">STD_LOGIC</span>;RCO: <span class="hljs-keyword">OUT</span> <span class="hljs-built_in">STD_LOGIC</span>;QD: <span class="hljs-keyword">OUT</span> <span class="hljs-built_in">STD_LOGIC</span>;QC: <span class="hljs-keyword">OUT</span> <span class="hljs-built_in">STD_LOGIC</span>;QB: <span class="hljs-keyword">OUT</span> <span class="hljs-built_in">STD_LOGIC</span>;QA: <span class="hljs-keyword">OUT</span> <span class="hljs-built_in">STD_LOGIC</span>);<span class="hljs-keyword">END</span> <span class="hljs-keyword">COMPONENT</span>;<span class="hljs-keyword">signal</span> rco_wire : <span class="hljs-built_in">std_logic_vector</span>(<span class="hljs-number">2</span> <span class="hljs-keyword">downto</span> <span class="hljs-number">0</span>);<span class="hljs-keyword">signal</span> useless: <span class="hljs-built_in">std_logic</span>; <span class="hljs-comment">-- 实例化的时候有一个输出管脚没有用到，但我不太会设置这个空管脚，就直接写了一个没有用到的信号</span><span class="hljs-keyword">begin</span>    <span class="hljs-comment">-- 以下都是f74161的实例化，需要设置port map，里面signal的顺序就是上面声明时port的顺序</span>counter0 : f74161 <span class="hljs-keyword">port</span> <span class="hljs-keyword">map</span>(CLR, LOAD, D(<span class="hljs-number">3</span>), D(<span class="hljs-number">2</span>), D(<span class="hljs-number">1</span>), E, D(<span class="hljs-number">0</span>), E, CPuPC, rco_wire(<span class="hljs-number">0</span>), Q(<span class="hljs-number">3</span>), Q(<span class="hljs-number">2</span>), Q(<span class="hljs-number">1</span>), Q(<span class="hljs-number">0</span>));counter1 : f74161 <span class="hljs-keyword">port</span> <span class="hljs-keyword">map</span>(CLR, LOAD, D(<span class="hljs-number">7</span>), D(<span class="hljs-number">6</span>), D(<span class="hljs-number">5</span>), rco_wire(<span class="hljs-number">0</span>), D(<span class="hljs-number">4</span>), rco_wire(<span class="hljs-number">0</span>), CPuPC, rco_wire(<span class="hljs-number">1</span>), Q(<span class="hljs-number">7</span>), Q(<span class="hljs-number">6</span>), Q(<span class="hljs-number">5</span>), Q(<span class="hljs-number">4</span>));counter2 : f74161 <span class="hljs-keyword">port</span> <span class="hljs-keyword">map</span>(CLR, LOAD, D(<span class="hljs-number">11</span>), D(<span class="hljs-number">10</span>), D(<span class="hljs-number">9</span>), rco_wire(<span class="hljs-number">1</span>), D(<span class="hljs-number">8</span>), rco_wire(<span class="hljs-number">1</span>), CPuPC, rco_wire(<span class="hljs-number">2</span>), Q(<span class="hljs-number">11</span>), Q(<span class="hljs-number">10</span>), Q(<span class="hljs-number">9</span>), Q(<span class="hljs-number">8</span>));counter3 : f74161 <span class="hljs-keyword">port</span> <span class="hljs-keyword">map</span>(CLR, LOAD, D(<span class="hljs-number">15</span>), D(<span class="hljs-number">14</span>), D(<span class="hljs-number">13</span>), rco_wire(<span class="hljs-number">2</span>), D(<span class="hljs-number">12</span>), rco_wire(<span class="hljs-number">2</span>), CPuPC, useless, Q(<span class="hljs-number">15</span>), Q(<span class="hljs-number">14</span>), Q(<span class="hljs-number">13</span>), Q(<span class="hljs-number">12</span>));<span class="hljs-keyword">end</span> rtl;</code></pre><hr /><ul><li><a href="https://www.nandland.com/vhdl/tutorials/index.html">参考</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;at-the-beginning&quot;&gt;at the beginning&lt;/h1&gt;
&lt;p&gt;the differences between VHDL and software programming languages&lt;/p&gt;
&lt;h2 id=&quot;serial-vs.-</summary>
      
    
    
    
    <category term="Programming language" scheme="http://furyton.github.io/categories/Programming-language/"/>
    
    <category term="Computer Organization" scheme="http://furyton.github.io/categories/Computer-Organization/"/>
    
    
    <category term="VHDL" scheme="http://furyton.github.io/tags/VHDL/"/>
    
    <category term="note" scheme="http://furyton.github.io/tags/note/"/>
    
  </entry>
  
  <entry>
    <title>Logistic Regression and Maximum Entropy Model</title>
    <link href="http://furyton.github.io/2021/02/15/Logistic-regression/"/>
    <id>http://furyton.github.io/2021/02/15/Logistic-regression/</id>
    <published>2021-02-15T07:56:55.000Z</published>
    <updated>2021-02-16T10:53:33.302Z</updated>
    
    <content type="html"><![CDATA[<h1 id="logistic-regression">Logistic Regression</h1><p>简介：一种简单的统计分类方法，因为使用了Logistic函数得名。称作回归(Regression)的原因是，这个模型实质是在做函数的拟合问题。</p><h2 id="logistic-function">Logistic Function</h2><p><span class="math display">\[f(x)=\frac{1}{1+e^{-x}}\]</span></p><p>介于<span class="math inline">\(0\)</span> ~ <span class="math inline">\(1\)</span>之间</p><h2 id="二分类的logistic模型">二分类的Logistic模型</h2><p><span class="math display">\[P(Y=1|x)=\frac{exp(w\cdot x+b)}{1+exp(w\cdot x+b)}\]</span> <span class="math display">\[P(Y=0|x)=\frac{1}{1+exp(w\cdot x+b)}\]</span> <span class="math inline">\(w，x \in \mathbf{R}^n\)</span> , <span class="math inline">\(b \in \mathbf{R}\)</span></p><p>将 <span class="math inline">\(b\)</span> 并入到 <span class="math inline">\(w\)</span> 中，扩充 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(x\)</span> ，从而得到更简洁的表达 <span class="math display">\[P(Y=1|x)=\frac{exp(w\cdot x)}{1+exp(w\cdot x)}\]</span> <span class="math display">\[P(Y=0|x)=\frac{1}{1+exp(w\cdot x)}\]</span></p><h3 id="对模型的理解">对模型的理解</h3><h4 id="几率odd">几率(odd)</h4><p>记事件发生的概率为 <span class="math inline">\(p\)</span> ，那么事件发生的几率为 <span class="math inline">\(\frac{p}{1-p}\)</span></p><p>对数几率(log odd / logit 函数) <span class="math display">\[logit(p)=log\frac{p}{1-p}\]</span> 对于Logistic回归来说 <span class="math display">\[log\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\cdot x\]</span> 可以看出，这是在用线性模型来拟合logit函数，故称为“回归模型”</p><h4 id="logistic函数">Logistic函数</h4><p>另一个角度看，Logistic函数将原本取值在实数集上的变量投影到了<span class="math inline">\((0,1)\)</span> ，也就是一个概率所属的范围。</p><h3 id="模型的求解">模型的求解</h3><p>极大似然估计，采用数值分析的方法(梯度下降法，拟牛顿法等)求解参数向量 <span class="math inline">\(w\)</span></p><h3 id="多分类的logistic回归模型">多分类的Logistic回归模型</h3><p><span class="math inline">\(Y\)</span> 的取值范围是{1, 2, 3, ... , K} <span class="math display">\[P(Y=k|x)=\frac{exp(w_k \cdot x)}{1+\sum_{k=1}^{K-1}exp(w_k\cdot x)},\quad k=1,2,\dots, K-1 \]</span> <span class="math display">\[P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}exp(w_k\cdot x)}\]</span></p><h1 id="最大熵模型">最大熵模型</h1><p>简介：最大熵模型是根据最大熵原理得到的，简单说，在所有可能的概率模型中，熵最大的模型是最好的，因为它保留了最大的不确定性，所有不确定的部分都是接近"等可能的"，减小了对不确定因素的偏见。</p><h2 id="模型的定义">模型的定义</h2><p>利用最大熵原理，获得一个简单的分类模型。这里所求的模型是条件概率模型。根据给定的训练集，可以获得联合分布<span class="math inline">\(P(X,Y)\)</span>的经验分布，以及边缘分布<span class="math inline">\(P(X)\)</span>的经验分布，分别记为<span class="math inline">\(\tilde{P}(X,Y)\)</span> 和 <span class="math inline">\(\tilde{P}(X)\)</span> 。</p><h3 id="约束">约束</h3><p>约束就是分类问题中确定的条件，也就是输入与输出之间一些已知的事实。</p><p>约束用特征函数表示。 <span class="math display">\[f(x,y)=\left\{\begin{matrix}1 ,&amp; x和y满足某一事实\\ 0 ,&amp; 否则\end{matrix}\right.\]</span> 如果模型能够学习到数据中的信息，那么就可以<strong>假设</strong>特征函数关于经验分布和模型分布的期望是相等的。</p><p>关于经验分布的期望 <span class="math display">\[E_{\tilde{P}}(f)=\sum_{x,y}\tilde{P}(x,y)f(x,y)\]</span> 关于模型的条件分布的期望 <span class="math display">\[E_P(f)=\sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)\]</span> 两者应当相等 ，即 <span class="math display">\[E_{\tilde{P}}(f)=E_P(f)\]</span></p><p>另外，特征函数也可不只一个。</p><h3 id="熵">熵</h3><p>条件熵 <span class="math display">\[H(P)=-\sum_{x,y}\tilde{P}(x)P(y\,|\,x)\,logP(y\,|\,x)\]</span></p><h3 id="模型学习">模型学习</h3><p>约束最优化问题 <span class="math display">\[\max_{P}\quad H(P)=-\sum_{x,y}\tilde{P}(x)P(y\,|\,x)\,logP(y\,|\,x)\]</span></p><p><span class="math display">\[s.t.\quad E_{\tilde{P}}(f_i)=E_P(f_i),\ i=1,2,...,n\]</span></p><p><span class="math display">\[\sum_yP(y\,|\,x)=1\]</span></p><p>根据拉格朗日乘子法， <span class="math display">\[L(P,w)=-H(P)+w_0\left ( 1-\sum_yP(y\,|\,x)\right)+\sum_{i=1}^{n}w_i(E_{\tilde{P}}(f_i)-E_P(f_i))\]</span> 问题转化为 <span class="math display">\[\min_{P}\max_{w}L(P,w)\]</span> 对偶问题 <span class="math display">\[\max_{w}\min_{P}L(P,w)\]</span> 因为<span class="math inline">\(L(P,w)\)</span> 是 P 的凸函数，故原始问题和对偶问题是等价的。(但其实两者等价的条件我还没学。。。我不懂，我不会)</p><p>通过对偶问题里面的极小化，可以获得一个含参的分布 <span class="math display">\[P_w(y\,|\,x)=\frac{1}{Z_w(x)}exp\left ( \sum_{i=1}^{n}w_if_i(x,y) \right )\]</span></p><p><span class="math display">\[Z_w(x)=\sum_yexp\left ( \sum_{i=1}^{n}w_if_i(x,y) \right )\]</span></p><p>其中<span class="math inline">\(Z_w(x)\)</span> 是正规化因子，<span class="math inline">\(w_i\)</span>是参数、权值。</p><p>下面在进一步去求外面的极大化就可以把参数求出来。</p><p>PS：最后一步的极大化其实等价于直接利用得到的含参的分布去做极大似然估计。</p><h3 id="模型求解">模型求解</h3><ul><li>有一个改进的迭代尺度法，计算每次参数 <span class="math inline">\(w\)</span> 的增量 <span class="math inline">\(\delta\)</span> , 关注<span class="math inline">\(L(w+\delta)-L(w)\)</span> , 经过放缩确定下界， 为了得到更大的该变量。放缩是因为，原式太复杂。</li><li>拟牛顿法</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h1&gt;
&lt;p&gt;简介：一种简单的统计分类方法，因为使用了Logistic函数得名。称作回归(Regression)的原因是，这个模型实质是在做函数的拟合问题。&lt;/p&gt;
&lt;h2 i</summary>
      
    
    
    
    <category term="Lab" scheme="http://furyton.github.io/categories/Lab/"/>
    
    <category term="Basic" scheme="http://furyton.github.io/categories/Lab/Basic/"/>
    
    
    <category term="Basic" scheme="http://furyton.github.io/tags/Basic/"/>
    
    <category term="Study" scheme="http://furyton.github.io/tags/Study/"/>
    
  </entry>
  
  <entry>
    <title>凸函数</title>
    <link href="http://furyton.github.io/2021/01/27/%E5%87%B8%E5%87%BD%E6%95%B0/"/>
    <id>http://furyton.github.io/2021/01/27/%E5%87%B8%E5%87%BD%E6%95%B0/</id>
    <published>2021-01-27T13:25:10.000Z</published>
    <updated>2021-02-27T14:40:26.649Z</updated>
    
    <content type="html"><![CDATA[<p>参考书 Boyd <em>Convex Optimization</em></p><h2 id="基本的概念和性质">基本的概念和性质</h2><h3 id="什么是凸函数">什么是凸函数</h3><p><span class="math inline">\(f:\mathbf {R}^n \rightarrow \mathbf {R}, \quad \mathbf {dom} f\)</span> 是个凸集，<span class="math inline">\(0 \leq \theta \leq 1\)</span> <span class="math display">\[f(\theta x + (1-\theta)y)\leq \theta f(x) + (1-\theta)f(y)\]</span> 严格凸就是不会取到等号</p><p>凹函数就是凸函数加个负号</p><p>仿射函数是比较特殊的，它既是凸函数又是凹函数，反过来也成立。相当于凹函数和凸函数的一个共有的临界的函数。</p><h4 id="扩展值延伸">扩展值延伸</h4><p>为了让凸函数在整个实数集上有定义，在定义域外令值为无穷，具体的为正无穷<span class="math inline">\(+\infty\)</span>(相反，若为凹函数，则定义为负无穷<span class="math inline">\(-\infty\)</span>) <span class="math display">\[\tilde{f}(x)=\left\{\begin{matrix}f(x) &amp;x\in\mathbf{dom}f\\ \infty &amp; x\notin \mathbf{dom}f\end{matrix}\right.\]</span></p><h3 id="判定条件">判定条件</h3><h4 id="一阶条件">一阶条件</h4><p>假设 <span class="math inline">\(f\)</span> 可微，对 <span class="math inline">\(\forall x,y \in \mathbf{dom}f\)</span> <span class="math display">\[f(y) \geqslant  f(x) + \bigtriangledown f(x)^T(y-x)\]</span> 联系了局部信息和全局信息</p><p>严格凸及凹的情况略</p><h4 id="二阶条件">二阶条件</h4><p>假设二阶可微，其Hessian矩阵是半正定的</p><p>未完待续。。。。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;参考书 Boyd &lt;em&gt;Convex Optimization&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;基本的概念和性质&quot;&gt;基本的概念和性质&lt;/h2&gt;
&lt;h3 id=&quot;什么是凸函数&quot;&gt;什么是凸函数&lt;/h3&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(f</summary>
      
    
    
    
    <category term="Lab" scheme="http://furyton.github.io/categories/Lab/"/>
    
    <category term="Basic" scheme="http://furyton.github.io/categories/Lab/Basic/"/>
    
    <category term="Optimization" scheme="http://furyton.github.io/categories/Optimization/"/>
    
    
    <category term="Basic" scheme="http://furyton.github.io/tags/Basic/"/>
    
    <category term="Study" scheme="http://furyton.github.io/tags/Study/"/>
    
  </entry>
  
  <entry>
    <title>FFT</title>
    <link href="http://furyton.github.io/2020/12/02/FFT/"/>
    <id>http://furyton.github.io/2020/12/02/FFT/</id>
    <published>2020-12-02T14:50:35.000Z</published>
    <updated>2020-12-07T16:04:33.830Z</updated>
    
    <content type="html"><![CDATA[<p>note : 本文仅简要地介绍FFT以及它的简单应用, 并不会过多的进行数学推导</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>首先我们回忆一下傅里叶级数.</p><h5 id="实数上的情形"><a href="#实数上的情形" class="headerlink" title="实数上的情形"></a>实数上的情形</h5><p>对于一个函数$f(x)$, 我们现在关注它在$[-\pi,\pi]$ 这个区间上的表现. </p><p>它能够写成(在一定条件下)</p><script type="math/tex; mode=display">f(x) = \frac{a_0}{2} + \sum_{k=1}^{\infty} a_kcos(kx)+b_ksin(kx)</script><p>这里</p><script type="math/tex; mode=display">\begin{align*}a_k = \frac{1}{\pi}\int_{-\pi}^{\pi} f(x)cos(kx)dx \\\\b_k = \frac{1}{\pi}\int_{-\pi}^{\pi} f(x)sin(kx)dx\end{align*}</script><p>虽然这里我们考虑的是在区间$[-\pi,\pi]$ 上, 但现在 $f$ 现在在整个$\textbf{R}$ 具有了周期性, 周期是$2\pi$. </p><p>另一方面, $f(x)$ 要满足”平方可积”, 详细来说, 所有平方可积的函数构成一个线性空间. 更进一步的有关希尔伯特空间等泛函分析的内容. </p><p>那这个线性空间自然具有一个无限维的基. 在傅里叶级数中, 选择的基是</p><script type="math/tex; mode=display">\\{ 1, cos(x), sin(x), cos(2x), sin(2x), \dots \\}</script><p>在确定内积</p><script type="math/tex; mode=display">\left \langle f,g\right \rangle = \int_{-\pi}^{\pi}f(x)\overline{g(x)}dx</script><p>后,这组由 $cos(kx),sin(kx)\dots$ 构成的基成为了一组正交基. 当然你可以选择规范化, 去除以每个的范数. </p><h5 id="复数上的情形"><a href="#复数上的情形" class="headerlink" title="复数上的情形"></a>复数上的情形</h5><p>把实数上的情形推广到复数上, 这里是说$f(x)$ 现在是复数函数, (x依然是实数). </p><p>我们有了更一般一点的傅里叶级数</p><script type="math/tex; mode=display">f(x) = \sum_{k=-\infty}^{+\infty} c_k e^{ikx}</script><p>这里</p><script type="math/tex; mode=display">c_k = \frac{\left \langle f(x), e^{ikx} \right\rangle}{\left \|e^{ikx} \right \|^2} = \frac{1}{2\pi}\int_{-\pi}^{\pi} f(x)e^{-ikx}dx</script><p>(注: Euler公式 $e^{ix}=cos(x) + i sin(x) ,\quad i=\sqrt{-1}$)</p><p>仍然是用许多的$cos(kx), sin(kx)$去表达$f(x)$, 只不过变成了复数. </p><p>前面一直讲的是$f(x)$在$[-\pi,\pi]$上的一些事情, 实际上这并不是唯一的. </p><p>现在换成区间$[-L,L]$ 做个映射, $x \to \pi x/L$</p><p>记</p><script type="math/tex; mode=display">\psi_k(x)=e^{i\pi kx/L}</script><p>有</p><script type="math/tex; mode=display">c_k = \frac{\left \langle f(x), \psi_k(x) \right\rangle}{\left \|\psi_k(x) \right \|^2} = \frac{1}{2L}\int_{-L}^{L} f(x)\psi_k(x)dx</script><p>所以</p><script type="math/tex; mode=display">f(x) = \sum_{k=-\infty}^{\infty}c_k\psi_k(x)=\frac{1}{2L}\sum_{k=-\infty}^{\infty}\left \langle f(x), \psi_k(x) \right\rangle \psi_k(x)</script><p>这里观察这个级数, 它实际是在将$f(x)$ 分解成一系列$\psi_k(x)$ 的线性组合. 或者说一系列$cos(2\pi kx/L), sin(2\pi kx/L)$ 的结合. 这里面$k$ 蕴含着不同的正弦函数的频率. </p><p>记</p><script type="math/tex; mode=display">\omega_k = \pi k/L \\\\\Delta \omega = \pi/L</script><p>那么</p><script type="math/tex; mode=display">\begin{align*}c_k = \frac{1}{2L} \int_{-L}^{L} f(x)& e^{-ik{\Delta\omega}x}dx = \frac{1}{2L}\int_{-L}^{L} f(x)e^{-i\omega_kx}dx \\\\f(x)=& \sum_{k=-\infty}^{+\infty}c_k e^{i\omega_k x}=\sum_{k=-\infty}^{+\infty}c_k e^{ik\Delta \omega x}\end{align*}</script><p>每个$c_k$都对应着一个$\omega_k$, 表示这个频率的函数所占的比重. </p><p>当$L \to +\infty$, 也就是说$f(x)$ 在$\textbf{R}$ 上非周期. 我们就得到了傅里叶变换.</p><script type="math/tex; mode=display">\begin{align*}\hat f(\omega) & = \mathcal{F(f(x))}=\int_{-\infty}^{\infty} f(x)e^{-i\omega x}dx\\\\f(x) =& \mathcal{F^{-1}(\hat f(\omega))} = \frac{1}{2\pi}\int_{-\infty}^{\infty} \hat f(\omega)e^{i\omega x}d\omega\end{align*}</script><p>有趣的性质:</p><script type="math/tex; mode=display">\begin{align*}& \mathcal{F(\frac{d}{dx}f(x)) = i\omega F(f(x))}\\\\\\\\& \mathcal{F(g\times f) = F(g)F(f)}\end{align*}</script><h2 id="DFT"><a href="#DFT" class="headerlink" title="DFT"></a>DFT</h2><p>对于计算机, 我们既不能方便的处理无穷级数, 也不好处理积分. 连续的情况对计算机来说比较困难. 所以出现了离散的傅里叶变换. 他的大致含义还是一样的.</p><p>对一个$f(x)$进行采样, 在0, 1, 2, … , n - 1处的值, 分别为$[f<em>0, f_1,f_2,…,f</em>{n-1}]$ , 这个向量代替了原来的函数</p><p>经过傅里叶变换后的函数变成了 $[\hat f<em>0, \hat f_1, \hat f_2…\hat f</em>{n-1}]$</p><p>对于 $\omega$ 的选取也有原先连续的情况变成离散. 将 $2\pi$ 平均分成了 n 份. 每一份长度是 $\frac{2\pi}{n}$ 作为一个基本的单元. </p><p>之前的函数 $\psi_k(x)$ 成为了 $[(e^{2\pi ik/n})^j], j=0,1,2,…,n-1$ </p><p>简单来说, DFT的形式就是</p><script type="math/tex; mode=display">\begin{align*}\hat f_k=\sum_{j=0}^{n-1}f_j e^{-2\pi ijk/n}\\\\f_k = \frac{1}{n}\sum_{j=0}^{n-1}\hat f_k e^{2\pi ijk/n}\end{align*}</script><p>注意 $e^{2\pi ik/n},\quad k=0,1,2…,n-1$ 就是 $x^{n}=1$ 的解</p><p>$\psi_k = \left[<br> \begin{matrix}<br>   1 \\<br>   e^{2\pi ik/n} \\<br>   (e^{2\pi ik/n})^2 \\<br>   \vdots \\<br>   (e^{2\pi ik/n})^{n-1}<br>  \end{matrix}<br>  \right]$ 可以看作是不同频率的函数, 那么$\hat f_k$ 便可以看作是对应的$\psi_k$ 所占的大小.</p><p>首先举个简单的例子去直观的理解这个变换.</p><p>我们在函数$f(x)=cos(2\pi \times 50 x)+cos(2\pi \times 75x)$ 上进行采样, 同时我们加上一个随机的噪声,范围是$[-4,4]$</p><div align=center><img src="\upload_image\original.jpg" alt="original" /></div><p>黑色的是原图像, 蓝色的部分是加入噪声后的图象. 当然这里的数据都是离散的, 只不过画图的时候被连在了一起.看上去像连续的.</p><p>现在我们得到了采样后的一个$f(x)$的向量, $[f<em>0,f_1,f_2,…,f</em>{n-1}]$ </p><p>经过DFT后, 我们能得到一个$\hat f$ 向量, 由于是复数, 这里只展示向量中每个复数的模长的平方. 可以看作是一种能量. </p><div align=center><img src="\upload_image\after_FFT.jpg" alt="after FFT" /></div><p>下面的图是经过DFT后的样子, 明显可以看到, 里面有两个非常高的点. 对应着原始的没有加入噪声的$cos(2\pi \times 50 x)$ 和 $cos(2\pi \times 75x)$ , 如果我们设置一个阈值, 小于100的置位0, 这样就能去除噪声.</p><div align=center><img src="\upload_image\after.jpg" alt="filtered" /></div><p>中间的图就是对$\hat f$ 进行过滤之后再经过DFT的逆操作的得到的图象, 会发现它完全去除了噪声, 还原了最初的$f(x)$ .</p><h3 id="计算DFT"><a href="#计算DFT" class="headerlink" title="计算DFT"></a>计算DFT</h3><p>下面的问题是如何去计算DFT呢,</p><script type="math/tex; mode=display">\begin{align*}\hat f_k=\sum_{j=0}^{n-1}f_j e^{-2\pi ijk/n}\\\\f_k = \frac{1}{n}\sum_{j=0}^{n-1}\hat f_k e^{2\pi ijk/n}\end{align*}</script><p>容易看到这实质上是一个矩阵乘法.</p><p>记 $\omega_n = e^{-2\pi i/n}$</p><script type="math/tex; mode=display">\hat f=\left [ \begin{matrix} \hat f_0 \\\\ \hat f_1 \\\\ \hat f_2 \\\\ \vdots \\\\ \hat f_{n-1}\end{matrix} \right ]= \left [ \begin{matrix} 1 & 1 & 1 & 1 & \dots & 1 \\\\1 & \omega_n & \omega_n^{2} & \omega_n^{3} & \dots & \omega_n^{n-1}\\\\1 & \omega_n^{2} & \omega_n^{4} & \omega_n^{6} & \dots & \omega_n^{2(n-1)} \\\\  &   & \vdots &  & \dots &\vdots \\\\1 & \omega_n^{n-1} & \omega_n^{2(n-1)} & \omega_n^{3(n-1)} & \dots & \omega_n^{(n-1)(n-1)}\end{matrix}\right]\left[ \begin{matrix}f_0 \\\\  f_1 \\\\  f_2 \\\\ \vdots \\\\  f_{n-1}\end{matrix}\right]</script><p>记这个矩阵为$\mathcal{F_n}$ , 或者称为DFT矩阵.</p><p>实质上$\mathcal{F}$ 乘上它的共轭转置  $\mathcal{F^*}$ 等于 $nE_n$， 所以它的逆矩阵是很好求的，DFT的逆变换也很容易得出.</p><p>如果直接去求这个矩阵，复杂度是比较高的$O(n^2)$ </p><p>但是如果考虑到 $\omega_n$ 的特殊的性质，便能得到非常高效的$O(nlogn)$ 的算法，也就是FFT</p><h3 id="FFT"><a href="#FFT" class="headerlink" title="FFT"></a>FFT</h3><p>FFT就是一种计算DFT的高效的算法，它最初是由高斯提出.</p><p>简单来说，我们想要快速的计算矩阵$\mathcal{F<em>n}$ ，通过将他分解为更小的子问题，转而去求解$\mathcal{F</em>{\frac{n}{2}}}$. 这里我们现在只考虑 n 是2的幂的情况. 如果n不是2的幂的话， 我们可以简单的扩充成2的幂， 并不会影响算法的效率.</p><p>不同的矩阵$\mathcal{F<em>n}$ 内所使用的$\omega_n$ 是不同的， 但对于n 和 n/2 我们有 $\omega_n^2=\omega</em>{n/2}$ 可以轻松的进行转化。</p><p>核心的想法就是对$[f<em>0, f_1,…,f</em>{n-1}]$ 进行重新排序， 按照下标的奇偶进行分组.</p><p> 对于多项式</p><script type="math/tex; mode=display">\begin{align*}p(x) =& a_0+a_1x+a_2x^2+\dots+a_{n-1}x^{n-1} \\\\     =& (a_0 + a_2x^2 + a_4x^4 + \dots + a_{n-2}x^{n-2})\\\\      &+ x(a_1 + a_3x^2+a_5x^4+\dots a_{n-1}x^{n-2})\\\\     =& E(x) +xO(x)\end{align*}</script><p>同理，对于这里的DFT， 我们就会的到</p><script type="math/tex; mode=display">\begin{align*}\hat f_k =& \sum_{j=0}^{n-1}f_j(\omega_n^k)^j\\\\         =& \sum_{j=0}^{n/2-1}f_{2j}(\omega_n^k)^{2j} +\omega_n^k\sum_{j=0}^{n/2-1}f_{2j+1}(\omega_n^k)^{2j}\\\\         =& \sum_{j=0}^{n/2-1}f_{2j}(\omega_{n/2}^k)^{j} +\omega_n^k\sum_{j=0}^{n/2-1}f_{2j+1}(\omega_{n/2}^k)^{j}\\\\         =& E + \omega_n^k O\end{align*}</script><p>$E$ 和 $O$ 便是我们的两个相同的子问题.</p><p>如果我们分别利用 $f<em>{even}$ 和 $f</em>{odd}$ 计算出了$E$ 和 $O$ 我们便能很快的根据上面的式子计算出所有的$\hat f_k$ </p><p>不仅如此，我们可以利用</p><script type="math/tex; mode=display">\omega_n^{k+n/2} = -\omega_n^{k}</script><p>进一步的简化.</p><script type="math/tex; mode=display">\begin{align*}\hat f_k=E+\omega_n^kO\\\\\hat f_{k+n/2}=E-\omega_n^kO\end{align*}</script><p>这样只需要一半的循环，就能计算出全部的$\hat f_k$ </p><script type="math/tex; mode=display">\begin{align*}\hat f=\left [ \begin{matrix} \hat f_0 \\\\ \hat f_1 \\\\ \hat f_2 \\\\ \vdots \\\\ \hat f_{n-1}\end{matrix} \right ]=&\mathcal{F_n}\left[ \begin{matrix}f_0 \\\\  f_1 \\\\  f_2 \\\\ \vdots \\\\  f_{n-1}\end{matrix}\right]\\\\=&\left[ \begin{matrix}E_{n/2} & D_{n/2} \\\\\\\\E_{n/2} & -D_{n/2}\end{matrix}\right]\left[ \begin{matrix}\mathcal{F_{n/2}} & O \\\\\\\\O & \mathcal{F_{n/2}}\end{matrix}\right]\left[ \begin{matrix}f_{even}\\\\\\\\f_{odd}\end{matrix}\right]\end{align*}</script><p>如此算法的复杂度达到了$O(nlogn)$ ，如果利用并行计算，可以达到更快.</p><h3 id="FFT的应用"><a href="#FFT的应用" class="headerlink" title="FFT的应用"></a>FFT的应用</h3><ul><li>像前面做过的去除噪声便是一个很常见的应用</li><li>图象压缩</li><li>多项式乘法或者大整数乘法</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;note : 本文仅简要地介绍FFT以及它的简单应用, 并不会过多的进行数学推导&lt;/p&gt;
&lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Backgrou</summary>
      
    
    
    
    
    <category term="Basic" scheme="http://furyton.github.io/tags/Basic/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec</title>
    <link href="http://furyton.github.io/2020/11/11/Word2Vec/"/>
    <id>http://furyton.github.io/2020/11/11/Word2Vec/</id>
    <published>2020-11-10T16:03:16.000Z</published>
    <updated>2020-11-10T17:57:16.656Z</updated>
    
    <content type="html"><![CDATA[<h3 id="beginning"><a href="#beginning" class="headerlink" title="beginning"></a>beginning</h3><p>其实用向量来表示一个单词是很常见的, 毕竟方便一个算法或模型去表示一个单词. 但表示的方法很重要, 最完美的Embedding就是能够包含单词的语义, 相似语义的单词的表示越接近, 反之表示的向量差距越大. 下面一步步的去完成我们的目标.</p><h4 id="distance"><a href="#distance" class="headerlink" title="distance"></a>distance</h4><p>怎么来叙述两个单词的embedding是相近的呢? 容易想到对于两个向量, 我们有一个适用于 $n$ 维空间上的一个夹角公式.<br> for $\vec{a}$, $\vec{b}$ as two embedding vectors of different words. the cosine similarity is<br>$$<br>Cosine Similarity(\vec{a}, \vec{b}) = \frac{\vec{a} \cdot \vec{b}}{|\vec{a}|\cdot|\vec{b}|}<br>$$</p><p>这个评判函数相对简单, 也非常有效. 对于不同的问题, 我们也可以选择不同的距离函数.</p><h4 id="modeling"><a href="#modeling" class="headerlink" title="modeling"></a>modeling</h4><p>embedding都是为了更好的完成一个task, 那么下面先来看一个简单的任务, 在完成任务的同时, 来获得一个适合它的embedding. </p><h5 id="predicting-next-word"><a href="#predicting-next-word" class="headerlink" title="predicting next word"></a>predicting next word</h5><p>根据语境预测下一个单词是什么. 这是一个在日常生活中非常常见的任务.<br>简单来做, 就是分为三步, </p><ul><li>输入一个单词, 先获得对应的embedding</li><li>利用embedding去预测下一个单词的embedding</li><li>根据embedding映射到对应的单词, 然后输出</li></ul><ul><li>数据集从哪来呢<ul><li>一般是Wikipedia的文章等等</li><li>构造许多定长的input - output对, 做法是window sliding with a fixed length, <strong>前面的都是input, 最后一个单词是output</strong></li></ul></li></ul><h5 id="problem"><a href="#problem" class="headerlink" title="problem"></a>problem</h5><p>这么做有两个问题. </p><ul><li><p>预测一个单词怎么能只看前几个单词呢</p></li><li><p>最后有个 $n \times d$ , (其中 $n$ 表示Vacab的大小, $d$ 表示embedding的大小) , 这算起来太费时了</p></li></ul><h5 id="problem-solving"><a href="#problem-solving" class="headerlink" title="problem solving"></a>problem solving</h5><ul><li><p>取样时不仅取前面的, 把后面的也取了. 这叫skip gram.</p><ul><li>这就成了Clozing了</li><li>换个方法, 把他改成预测谁是neighbor, 输出可能是neighbor的概率</li></ul></li><li><p>computational problem咋整. 再调整我们的任务</p><ul><li><p>我们改成更小规模的模型, 给他一个input, 和一堆output, 然后输出这些output分别是不是他的neighbor</p></li><li><p>那有可能我们训出了一个人工智障, 他告诉我们这些全都是neighbor, 我们好像也没法反驳.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># are they neighbors? </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span>(<span class="params"><span class="built_in">input</span>, output</span>):</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">1.0</span></span><br></pre></td></tr></table></figure></li><li><p>给他来个negative sample, 告诉model他们<strong>不是</strong>neighbor, 这样model就成了个logistics regression模型, 规模小了很多</p></li><li><p>但咱采样的时候怎么知道他们到底有没有可能是neighbor, 我们手上的数据集肯定是不完备的呀</p></li><li><p>随机设为negative……</p></li></ul></li></ul><p>问题基本解决了, 下面看具体流程</p><h5 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h5><ul><li><p>我们训两个embeddings, 叫做Embedding和Context, 分别为input和一组outputs做project. 获得两组embedding: $\vec{input}$ 和 $\vec{output}$. 这个outputs肯定就是一些真正的neighbor(通过skip gram window sliding选出来的)加上一些随机的negative sample.</p></li><li><p>input $\to$ Embedding $\to$ $\vec{input}$</p><p>outputs $\to$ Context $\to$  a set of $\vec{output}$s</p></li><li><p>然后点乘, 来个softmax, 获得这么个概率, 根据误差不断训</p></li><li><p>最后, 训得差不多了, 扔掉context, 这个Embedding就是炼出来的丹</p></li></ul><h4 id="last"><a href="#last" class="headerlink" title="last"></a>last</h4><ul><li>语义相近的那些单词, 被认为是所在的Context是相似的. 越相似的词点乘上Context, 得到的结果肯定都很接近正确结果.</li><li>这个window sliding 的length一般设成5, 越小的话得到的embedding划的越细, 就是说相近的embedding的单词所在的context几乎一样, 但要注意, 反义词很多时候也是这样, 把length设大点就能区分更多语境</li></ul><h3 id="End"><a href="#End" class="headerlink" title="End"></a>End</h3><h5 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h5><ul><li><a href="http://jalammar.github.io/illustrated-word2vec/">a blog</a></li><li>还没看过原paper和代码, 估计等以后了</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;beginning&quot;&gt;&lt;a href=&quot;#beginning&quot; class=&quot;headerlink&quot; title=&quot;beginning&quot;&gt;&lt;/a&gt;beginning&lt;/h3&gt;&lt;p&gt;其实用向量来表示一个单词是很常见的, 毕竟方便一个算法或模型去表示一个单词. 但表示</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://furyton.github.io/categories/Machine-Learning/"/>
    
    <category term="Lab" scheme="http://furyton.github.io/categories/Lab/"/>
    
    <category term="Basic" scheme="http://furyton.github.io/categories/Lab/Basic/"/>
    
    
    <category term="Machine Learning" scheme="http://furyton.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="http://furyton.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>多边形的扫描转换与区域填充算法</title>
    <link href="http://furyton.github.io/2020/11/09/%E5%A4%9A%E8%BE%B9%E5%BD%A2%E7%9A%84%E6%89%AB%E6%8F%8F%E8%BD%AC%E6%8D%A2%E4%B8%8E%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85%E7%AE%97%E6%B3%95/"/>
    <id>http://furyton.github.io/2020/11/09/%E5%A4%9A%E8%BE%B9%E5%BD%A2%E7%9A%84%E6%89%AB%E6%8F%8F%E8%BD%AC%E6%8D%A2%E4%B8%8E%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85%E7%AE%97%E6%B3%95/</id>
    <published>2020-11-09T14:33:08.000Z</published>
    <updated>2021-03-06T03:24:20.842Z</updated>
    
    <content type="html"><![CDATA[<h1 id="多边形的扫描转换与区域填充算法">多边形的扫描转换与区域填充算法</h1><p>为了完成这次算法作业, 需要C++ 图形编程. 我选择了OpenGL. 配置起来有点麻烦, 又遇到了一些Bug, 最后索性用了GL里面的一个库, glut.h 进行尝试.</p><h6 id="cheating-code"><a href="https://blog.csdn.net/qq_34075012/article/details/53283372">Cheating Code</a></h6><h3 id="glut的学习">GLUT的学习</h3><p>用的Clion, 配置了库, 但是可能有问题, 至少Glut是能正常使用的</p><p>随便找的一篇博客<a href="https://blog.csdn.net/OOFFrankDura/article/details/80205718">配置OpenGL</a></p><p>随便找到的一篇<a href="https://www.academia.edu/3186255/GLUT_Tutorial">GLUT Tutorial</a></p><p>捣腾半天发现其实我需要的功能并不是很复杂, 也没用到太多glut.h更深入的东西. 了解了glut基本的结构以后还是比较容易看懂整体的流程的.</p><ul><li>main()</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c">glutInit(&amp;argc, argv);<span class="hljs-comment">// 初始化 glut</span><br><br>glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); <span class="hljs-comment">// 设置模式, 我现在只会选择单缓存还是双缓存, 或者深度缓存等等以及颜色</span><br>glutInitWindowSize(winX, winY);<br>glutCreateWindow(<span class="hljs-string">&quot;test&quot;</span>);<br>myInit(); <span class="hljs-comment">//设置背景之类的</span><br><br>glutDisplayFunc(display); <span class="hljs-comment">//这个传入的display貌似也被当做各种窗口变动recall函数</span><br><br>glutMouseFunc(processMouse);<br>glutKeyboardFunc(processKeyboard);<br><br>glutMainLoop();<br></code></pre></td></tr></table></figure><ul><li>myInit()</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c">glClearColor(<span class="hljs-number">255.0</span>, <span class="hljs-number">255.0</span>, <span class="hljs-number">255.0</span>, <span class="hljs-number">0.0</span>);<br>glClear(GL_COLOR_BUFFER_BIT);<br>glColor3f(<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>);<br><br>glPointSize(<span class="hljs-number">2.0</span>);<br>glMatrixMode(GL_PROJECTION);<br>glLoadIdentity();<br>gluOrtho2D(<span class="hljs-number">0</span>, winX, <span class="hljs-number">0</span>, winY); <span class="hljs-comment">// 类似于一个映射或投影之类的, 把最边界的地方所在的坐标确定, left, right, bottom, top, 这里这样就会让他变成常规的坐标系(原点在左下)</span><br><br>glFlush();<br></code></pre></td></tr></table></figure><ul><li>recall</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">drawLine</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x1, <span class="hljs-keyword">int</span> y1, <span class="hljs-keyword">int</span> x2, <span class="hljs-keyword">int</span> y2)</span> </span>&#123;<br>    glVertex2i(x1, y1);<br>    glVertex2i(x2, y2);<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">processKeyboard</span><span class="hljs-params">(<span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span> key, <span class="hljs-keyword">int</span> _x, <span class="hljs-keyword">int</span> _y)</span> </span>&#123;<br>    <span class="hljs-keyword">if</span>(key == <span class="hljs-number">27</span>) <span class="hljs-built_in">exit</span>(<span class="hljs-number">0</span>);<br>    <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-keyword">if</span>(n &gt;= <span class="hljs-number">3</span>) &#123;<br>            Filling f = Filling(n, x, y); <span class="hljs-comment">//这个就是本文所作的填充算法</span><br>            glBegin(GL_LINES);<br>            f.fill(drawLine);<br>            glEnd();<br>            glFlush();<br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">processMouse</span><span class="hljs-params">(<span class="hljs-keyword">int</span> button, <span class="hljs-keyword">int</span> state, <span class="hljs-keyword">int</span> _x, <span class="hljs-keyword">int</span> _y)</span> </span>&#123;<br>    <span class="hljs-keyword">if</span>(state == GLUT_DOWN) &#123;<br>        x[n] = _x; y[n++] = winY - _y;<br>        glBegin(GL_POINTS);<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) &#123;<br>            glVertex2i(x[i], y[i]);<br>        &#125;<br>        glEnd();<br>        glFlush();<br>    &#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">display</span><span class="hljs-params">()</span> </span>&#123;<br>    glClear(GL_COLOR_BUFFER_BIT);<br>    glClearColor(<span class="hljs-number">255.0</span>, <span class="hljs-number">255.0</span>, <span class="hljs-number">255.0</span>, <span class="hljs-number">0.0</span>);<br>    glColor3f(<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>);<br><br>    glBegin(GL_POINTS);<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) &#123;<br>        glVertex2i(x[i], y[i]);<br>    &#125;<br>    glEnd();<br>    glFlush();<br>&#125;<br></code></pre></td></tr></table></figure><p>to be continue...</p><p>算法代码之后再说....</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;多边形的扫描转换与区域填充算法&quot;&gt;多边形的扫描转换与区域填充算法&lt;/h1&gt;
&lt;p&gt;为了完成这次算法作业, 需要C++ 图形编程. 我选择了OpenGL. 配置起来有点麻烦, 又遇到了一些Bug, 最后索性用了GL里面的一个库, glut.h 进行尝试.&lt;/p&gt;</summary>
      
    
    
    
    <category term="Assign" scheme="http://furyton.github.io/categories/Assign/"/>
    
    
    <category term="图形学" scheme="http://furyton.github.io/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    <category term="数据结构" scheme="http://furyton.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Transformer模型学习笔记</title>
    <link href="http://furyton.github.io/2020/11/09/Transformer%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://furyton.github.io/2020/11/09/Transformer%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2020-11-08T17:22:25.000Z</published>
    <updated>2021-01-28T10:03:47.133Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer-模型学习笔记"><a href="#Transformer-模型学习笔记" class="headerlink" title="Transformer 模型学习笔记"></a>Transformer 模型学习笔记</h1><h6 id="论文地址Attention-Is-All-You-Need"><a href="#论文地址Attention-Is-All-You-Need" class="headerlink" title="论文地址Attention Is All You Need"></a>论文地址<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a></h6><h6 id="Nice-Blog-for-illustrating-Transformer-Model"><a href="#Nice-Blog-for-illustrating-Transformer-Model" class="headerlink" title="Nice Blog for illustrating Transformer Model"></a><a href="http://jalammar.github.io/illustrated-transformer/">Nice Blog for illustrating Transformer Model</a></h6><h6 id="seq2seq-with-attention"><a href="#seq2seq-with-attention" class="headerlink" title="seq2seq with attention"></a><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">seq2seq with attention</a></h6><h4 id="提出的特点"><a href="#提出的特点" class="headerlink" title="提出的特点"></a>提出的特点</h4><ul><li>RNN无法并行地去处理一个序列, 因为每个hidden state $h<em>i$都是依赖于上一个hidden state  $h</em>{i-1}$以及input. 所以就要一个step接一个step的去循环, 对于很长的序列训练起来就很耗时. Transformer 模型<strong>利用Attention机制</strong>去捕获全局的input与output之间的依赖性, 实质上就是将整条序列看作一个input向量, 也就<strong>避免</strong>了循环神经网络中的”<strong>循环</strong>“. 实质上算是对RNN循环过程的一个展开吧.</li><li><strong>完全使用Attention机制</strong>, 没有使用序列对齐的循环(sequence-aligned recurrence)或者卷积层</li></ul><h4 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h4><h5 id="Self-attention的优势"><a href="#Self-attention的优势" class="headerlink" title="Self attention的优势"></a>Self attention的优势</h5><ul><li>Gated RNNs 虽然在结构上能够记录前 $n - 1$ 个token的信息, 但实际上, 随着序列变长, 最早的token信息会变得很少, 这就会失去他的准确性, 这在翻译任务中就显得非常要命, 例如, 在English-to-French的翻译里, output的第一个词大概率是依赖于input开始的部分, 这样很可能会得到很差的结果. 而transformer模型似乎是靠着更大的存储和算力来强行将前 $n$ 个token利用attention融合起来. 这样看来, 似乎是对症下药, 实验上也得到了很好的结果.</li><li>而具体的self attention会在模型架构中介绍</li></ul><h5 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h5><h6 id="Blog-for-introducing-Word2Vec"><a href="#Blog-for-introducing-Word2Vec" class="headerlink" title="Blog for introducing Word2Vec"></a><a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca">Blog for introducing Word2Vec</a></h6><p>我自己的<a href="https://furyton.github.io/2020/11/11/Word2Vec/">对Word2Vec的学习笔记</a></p><h4 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h4><p>其实Transformer模型的改进应该去对比seq2seq with attention模型</p><ul><li><p>首先在seq2seq模型里,用的是最初的encoder-decoder思想, 拿翻译任务来说, input就是一个句子, encoder需要一个单词一个单词(token)的去encode, 也就是扔进一个RNNs, 每次得到一个hidden state, 句子全输进去了, 最后得到的hidden state就可以作为整个句子的representation. 这个思想很简单, 给我的是不定长的, 那我就把他搞成一个固定长度的hidden state. 之后拿着这个representation当作decoder的input, 再一个单词一个单词的预测. 前后都是RNN.</p><p>乍一看貌似还是挺好的, 但问题是RNN的记忆机制和梯度问题解决的不是那么好, 句子长了前面的单词他就忘记了. 而且翻译这个任务也确实需要一种attention ,output的某一个部分会很大程度依赖于input中的一部分. </p></li><li><p>加了attention的seq2seq似乎就考虑到了这种依赖关系, attention机制也很好的做到了这一点. 至于整体做法, 上面不是说encoder内不断地生成hidden state吗, 那我们就把它们全都取出来作为decoder的input, 这样就不用太担心记忆的问题了, 毕竟你把它们都拿出来了. 然后每个output预测值会利用attention机制去给这些hidden state附上注意力的权重, 来更好地完成任务. </p></li><li><p>再到transformer. 这样纵向的来看, 似乎改进的地方确实如原论文所讲, 去掉了所有的循环连接. 完全用attention来解决. 这样做就需要在一些地方进行调整.</p></li></ul><h5 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h5><p>虽然字面上讲的attention, 似乎是个很熟悉的概念, 但实际在具体的实现上是另一种更加抽象的机制.</p><p>一般来讲,这个问题是想输入两种序列, $S= {S<em>1, S_2, S_3 \dots S_n }$ 以及 $T ={T_1, T_2, T_3 \dots T_m }$ 我想输出$T$ 对于$S$ 的attention, 具体的可以说就是一个function, $Attention</em>{S_i}(T_j),\quad i= 1,2\dots n$</p><p>,然后有</p><script type="math/tex; mode=display">\sum_{j} Attention_{S_i}(T_j)=1, \quad i = 1,2,\dots n</script><p>就可以,值越大就越重要. 那么整个T对于 $S<em>i$ 的representation就是 $T</em>{s<em>i} = \sum</em>{j} Attention_{S_i} (T_j)\times Value(T_j)$ 这么一个加权平均</p><p>这里的元素就是一些embedding, 一些向量. </p><p>Attention的求法类似于一种查询. 每个 $S<em>i$ 都会对应一个query向量 $\boldsymbol{q_i}$ 每个 $T_i$ 又对应一个键值 $\boldsymbol{k_i}$ 以供”查询”, 查到的结果就是两个向量的点积 $a</em>{ij} = \boldsymbol{q_i} \cdot \boldsymbol{k_j}$ , (假设这里的两个向量的维数都是 $d_k$ ).</p><p>最后的Attention就是再加上一个softmax</p><script type="math/tex; mode=display">Attention_{S_i}(T_j)  = \frac{e^{\boldsymbol{q_i} \cdot \boldsymbol{k_j}^T}}{\sum_j e^{\boldsymbol{q_i} \cdot \boldsymbol{k_j}^T}}</script><p>每个 $T_j$ 又会对应一个Value向量 $v_j$ (维度可以和前面两个向量不同, 记为$d_v$)用以获得representation. 最后得到的就是</p><script type="math/tex; mode=display">Representation_{for\ S_i} = softmax(\boldsymbol{q}_{1\times d_k}\cdot \boldsymbol K_{m \times d_k}^T) \boldsymbol V_{m\times d_k}</script><p>进一步可以获得 $T$ 对 $S$ 的表示 </p><script type="math/tex; mode=display">softmax(\boldsymbol{Q}_{n\times d_k}\cdot \boldsymbol K_{m \times d_k}^T) \boldsymbol V_{m\times d_v}</script><p>为了”having more stable gradients” , 在$\boldsymbol{Q}\cdot \boldsymbol K^T$这里还要除以一个因子, 默认是 $\sqrt{d_k}$ ,</p><p>然后又变成了</p><script type="math/tex; mode=display">softmax(\frac{\boldsymbol{Q}\cdot \boldsymbol K^T}{\sqrt{d_k}}) \boldsymbol V</script><p>有人要问了, 你说的这些query,key和value向量都咋求呢. </p><ul><li>用三个线性映射(矩阵) $W^Q,W^K,W^V$</li></ul><p>线性映射哪来的呢</p><ul><li>学出来的</li></ul><h5 id="a-beast-with-multihead"><a href="#a-beast-with-multihead" class="headerlink" title="a beast with multihead"></a>a beast with multihead</h5><p>这样一组attention可能注意力太集中, 看不全, 那我们就让他有多个”头”, 注意力分散点, 看得更全</p><p><a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">tensor2tensor</a>上有个示例, 演示的就是attention</p><p>一个比较经典的例子 翻译句子 The animal didn’t cross the street because it was too tired</p><p>这里的 it 应该代指 The animal, 但是对模型来说, 他也可能是说the street.</p><div align=center><img src="\upload_image\attention_with_one_head.png" alt="attention with one head" style="zoom:50%;" /></div><p>这里可以看到, 模型确实被it的代指给弄晕了,但还好animal处的颜色比street的地方要深,说明他的权值要大</p><div align=center><img src="\upload_image\attention_with_one_head_2.png" alt="attention with one head(2)" style="zoom: 50%;" /></div><p>但并不是所有的attention都能学到对应的部分.</p><p>解决办法就是, 我们用多个attention去拼接成最终想要的representation. 具体的, 我们得到的value向量不是 $d<em>v$ 维的吗, 假设我们有 $h$ 个head, 那么就把向量分为$h$ 个维度为 $d_v / h$ 的向量, 每个用各自的线性映射得到 $h$ 组不同的 $\boldsymbol{Q,\, K,\, V</em>{m \times (d_v/h)}}$ 去求各自的value(attention结构图片来自<a href="http://jalammar.github.io/illustrated-transformer/">这个blog</a>)</p><div align=center><img src="\upload_image\multihead.png" alt="multihead attention" style="zoom:50%;" /></div><p>最后一般还会再乘上一个矩阵 $W^O$,来得到最后的输出</p><div align=center><img src="\upload_image\attention_with_multihead.png" alt="attention with multihead" style="zoom:50%;" /></div><p>用上了多个head, 我们就能同时去关注不同的区域, 获得更准确的表述</p><div align=center><img src="\upload_image\attention_with_3heads.png" alt="attention with 3 heads" style="zoom:50%;" /></div><h4 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h4><p>前面花了较长篇幅讲了Attention机制, 这里再看下它是如何被用在Transformer中的</p><div align=center><img src="\upload_image\transformer_model_arch.png" alt="transformer_model architecture" style="zoom:50%;"/></div><p>在上面的架构图中包含encoder以及decoder的结构(结构图来自原论文)</p><p>左边是encoder, 他的特点就是直接将整条序列直接放进网络层中.</p><p>工作流程就是, 首先把要处理的序列input输入, 再获得它的embeddings. 然后依次进入每个encoder层, </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Transformer-模型学习笔记&quot;&gt;&lt;a href=&quot;#Transformer-模型学习笔记&quot; class=&quot;headerlink&quot; title=&quot;Transformer 模型学习笔记&quot;&gt;&lt;/a&gt;Transformer 模型学习笔记&lt;/h1&gt;&lt;h6 id=&quot;论</summary>
      
    
    
    
    <category term="Machine Learning" scheme="http://furyton.github.io/categories/Machine-Learning/"/>
    
    <category term="Lab" scheme="http://furyton.github.io/categories/Lab/"/>
    
    <category term="Basic" scheme="http://furyton.github.io/categories/Lab/Basic/"/>
    
    
    <category term="Machine Learning" scheme="http://furyton.github.io/tags/Machine-Learning/"/>
    
    <category term="NLP" scheme="http://furyton.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>我的第一篇文章</title>
    <link href="http://furyton.github.io/2020/11/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/"/>
    <id>http://furyton.github.io/2020/11/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/</id>
    <published>2020-11-08T11:03:14.000Z</published>
    <updated>2020-11-08T11:04:39.405Z</updated>
    
    <content type="html"><![CDATA[<h1 id="this-is-my-first-blog-here"><a href="#this-is-my-first-blog-here" class="headerlink" title="this is my first blog here"></a>this is my first blog here</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;this-is-my-first-blog-here&quot;&gt;&lt;a href=&quot;#this-is-my-first-blog-here&quot; class=&quot;headerlink&quot; title=&quot;this is my first blog here&quot;&gt;&lt;/a&gt;this is </summary>
      
    
    
    
    <category term="test" scheme="http://furyton.github.io/categories/test/"/>
    
    
    <category term="test" scheme="http://furyton.github.io/tags/test/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://furyton.github.io/2020/11/08/hello-world/"/>
    <id>http://furyton.github.io/2020/11/08/hello-world/</id>
    <published>2020-11-08T10:48:32.418Z</published>
    <updated>2020-11-08T10:48:32.419Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
